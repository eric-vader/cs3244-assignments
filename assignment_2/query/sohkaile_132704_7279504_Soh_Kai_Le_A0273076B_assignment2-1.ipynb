{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a38d7d28",
      "metadata": {
        "id": "a38d7d28"
      },
      "source": [
        "# Assignment 2: Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a19dc9a3",
      "metadata": {
        "id": "a19dc9a3"
      },
      "source": [
        "Fill in your name and student ID here.\n",
        "- Name: Soh Kai Le\n",
        "- Student ID: A0273076B (E1122479)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e05743",
      "metadata": {
        "id": "37e05743"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4804e854",
      "metadata": {
        "id": "4804e854"
      },
      "source": [
        "In this assignment, we'll implement Decision Trees:\n",
        "\n",
        "1. **Decision Tree Regression**\n",
        "    - Compute Region-Residual Sum of Squares (Region-RSS)\n",
        "    - Decision Tree Regressor\n",
        "2. **Decision Tree Classification**\n",
        "    - Calculate Entropy\n",
        "    - Compute Information Gain\n",
        "    - Implement the Majority Class function\n",
        "    - Implement the Best Split function\n",
        "    - Implement the Recursive Tree Builder Function\n",
        "    - Implement Prediction Logic for a single data point\n",
        "    - Decision Tree Classifier\n",
        "3. **Practical**: Train a DT classifier on the training dataset using scikit-learn\n",
        "\n",
        "By the end, you'll understand how DT works and how to tackle a problem using DT. Letâ€™s dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66b23d61",
      "metadata": {
        "id": "66b23d61"
      },
      "source": [
        "## Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e36bcc6",
      "metadata": {
        "id": "3e36bcc6"
      },
      "source": [
        "1. Fill in your name and student ID at the top of the ipynb file.\n",
        "2. The parts you need to implement are clearly marked with the following:\n",
        "\n",
        "    ```\n",
        "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "\n",
        "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "    ```\n",
        "\n",
        "    , and you must **ONLY** write your code in between the above two lines.\n",
        "3. **IMPORTANT**: Make sure that all of the cells are runnable and can compile without exception, even if the answer is incorrect. This will significantly help us in grading your solutions.\n",
        "3. For task 1 and 2, you are only allowed to use basic Python functions in your code (no `NumPy` or its equivalents), unless otherwise stated. You may reuse any functions you have defined earlier. If you are unsure whether a particular function is allowed, feel free to ask any of the TAs.\n",
        "4. For task 3, you may use the `scikit-learn` library.\n",
        "5. Your solutions will be evaluated against a set of hidden test cases to prevent hardcoding of the answer. You may assume that the test cases are always valid, unless specified otherwise. Partial marks may be given for partially correct solutions.\n",
        "\n",
        "### Submission Instructions\n",
        "Items to be submitted:\n",
        "* **This notebook, NAME-STUID-assignment2.ipynb**: This is where you fill in all your code. Replace \"NAME\" with your full name and \"STUID\" with your student ID, which starts with \"A\", e.g. `\"John Doe-A0123456X-assignment2.ipynb\"`\n",
        "\n",
        "Submit your assignment by **Sunday, 14 September 23:59** to Canvas. Points will be deducted late submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b3fadaa-a686-498a-b9c0-d56a6c8b591f",
      "metadata": {
        "id": "2b3fadaa-a686-498a-b9c0-d56a6c8b591f"
      },
      "source": [
        "## Overview\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f8d142c",
      "metadata": {
        "id": "9f8d142c"
      },
      "source": [
        "## Task 1 - Decision Tree Regression [4 Points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fef572c3-37d7-4cb1-a9b3-e3c5c35f2586",
      "metadata": {
        "id": "fef572c3-37d7-4cb1-a9b3-e3c5c35f2586"
      },
      "source": [
        "### Task 1.1 - Compute Region-Residual Sum of Squares (Region-RSS) [1 Point]\n",
        "\n",
        "Minimizing $\\operatorname{Region-RSS}(l, c)$ helps us find the best split for the decision tree at each step.\n",
        "\n",
        "$$\n",
        "\\underbrace{\\operatorname{Region-RSS}(l,c)}_\\text{Assume feature $l$, cutoff $c$}= \\underbrace{\\operatorname{RSS}(\\{X|X_l<c\\})}_\\text{Left subregion} + \\underbrace{\\operatorname{RSS}(\\{X|X_l\\geq c\\})}_\\text{Right subregion}\n",
        "$$\n",
        "\n",
        "In order to do so, we need to calculate the residuals for each sub-region:\n",
        "\n",
        "$$\n",
        "\\operatorname{RSS}(X) = \\sum_{i=1}^{n} \\left(y_i - \\hat{f}(x_i)\\right)^2 = \\sum_{i=1}^{n} \\left(e_i\\right)^2\n",
        "$$\n",
        "\n",
        "Implement a function that computes the Region-RSS for a given cutoff, using the target values in the left and right sub-regions (`y_left` and `y_right`), without the use of `numpy`.\n",
        "\n",
        "**Note**: Make sure that your code is able to handle the case where either `y_left` or `y_right` is an empty list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ea197b46-1d86-4dbd-8474-36d62b8291d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea197b46-1d86-4dbd-8474-36d62b8291d1",
        "outputId": "01ba2d3a-b2cb-45da-c084-df6b9b477a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ],
      "source": [
        "# TASK 1.1\n",
        "def calculate_regionrss(y_left, y_right):\n",
        "    \"\"\"\n",
        "    TODO: Compute the Region-RSS for the split.\n",
        "    Avoid using NumPy and use only basic Python functions.\n",
        "\n",
        "    Args:\n",
        "        y_left: A list of target values in the left split\n",
        "        y_right: A list of target values in the right split\n",
        "\n",
        "    Returns:\n",
        "        Region-RSS error for a given cutoff\n",
        "    \"\"\"\n",
        "\n",
        "    total_rss = 0\n",
        "\n",
        "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "    # y_left and y_right are the yi values so we first need to find ymean per side then apply the formula\n",
        "    # so sum over the loop the squared values for both sides. Then total_rss = rss_left + rss_right\n",
        "    rss_left = 0\n",
        "    y_left_mean = 0\n",
        "    rss_right = 0\n",
        "    y_right_mean = 0\n",
        "\n",
        "    # Left region\n",
        "\n",
        "    y_left_length = len(y_left)\n",
        "    y_right_length = len(y_right)\n",
        "\n",
        "    if y_left_length != 0: # Ensure that it can handle the case where it is an empty list\n",
        "      for i in y_left:\n",
        "        y_left_mean += i\n",
        "\n",
        "      y_left_mean = y_left_mean / y_left_length\n",
        "\n",
        "      for j in y_left:\n",
        "        rss_left += (j - y_left_mean) ** 2\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    # Same for the right\n",
        "\n",
        "    if y_right_length != 0: # Ensure that it can handle the case where it is an empty list\n",
        "      for k in y_right:\n",
        "        y_right_mean += k\n",
        "\n",
        "      y_right_mean = y_right_mean / y_right_length\n",
        "\n",
        "      for l in y_right:\n",
        "        rss_right += (l - y_right_mean) ** 2\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    total_rss = rss_left + rss_right\n",
        "\n",
        "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "    return total_rss\n",
        "\n",
        "# TESTCASES 1.1\n",
        "import math\n",
        "\n",
        "assert math.isclose(calculate_regionrss([3, 4, 5], [8, 9]), 2.5, rel_tol=1e-5)\n",
        "assert math.isclose(calculate_regionrss([1, 1, 1], [1, 1]), 0.0, rel_tol=1e-5)\n",
        "assert math.isclose(calculate_regionrss([], [1, 1]), 0.0, rel_tol=1e-5)\n",
        "print('All test cases passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "403f06f5",
      "metadata": {
        "id": "403f06f5"
      },
      "source": [
        "### Task 1.2 - Decision Tree Regressor [3 Points]\n",
        "\n",
        "Our Decision Tree Regressor recursively splits the data into two regions at each node, using binary splits that minimize the Region-RSS. Splitting stops when the maximum depth is reached or there is no possible/valid splitting. Each leaf predicts the mean target value of its region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "101dbdca",
      "metadata": {
        "id": "101dbdca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8708110-3855-4ab9-dd7f-e6c052381c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ],
      "source": [
        "# TASK 1.2\n",
        "class DTRegressor:\n",
        "    def __init__(self, max_depth=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X: A list of feature values (one feature per sample).\n",
        "            y: A list of target values corresponding to each sample.\n",
        "        \"\"\"\n",
        "        self.tree = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        \"\"\"\n",
        "        TODO: Implement the recursive tree building logic using RSS.\n",
        "        Hint: Add to right node when it is exactly the same as the split value.\n",
        "\n",
        "        Args:\n",
        "            X: A list of feature values (one feature per sample)\n",
        "            y: A list of target values corresponding to each sample\n",
        "            depth: The current depth of the tree\n",
        "\n",
        "        Returns:\n",
        "            dict or float:\n",
        "                If it's an internal node, returns a dictionary representing the node:\n",
        "                - 'split_value': The feature value at which the split occurs.\n",
        "                - 'left': The left child node (recursively built tree structure or leaf value).\n",
        "                - 'right': The right child node (recursively built tree structure or leaf value).\n",
        "                If it's a leaf node (base case) or max_depth reached, returns the mean of the target values\n",
        "                in that region, which will be the prediction for that leaf.\n",
        "        \"\"\"\n",
        "\n",
        "        res = None\n",
        "\n",
        "        \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "        # Testing\n",
        "        # print(calculate_regionrss([2,4,6,8,10], []))\n",
        "\n",
        "        # For X = [1,2,3,4,5]\n",
        "\n",
        "        # [], [2,4,6,8,10] gives 40 corresponds to y[:0], split value = 1 / 2 = 0.5\n",
        "        # [2], [4,6,8,10] gives 20 corresponds to y[:1], split value = (1+2)/2 = 1.5, corresponding index = 1 which is value = 2\n",
        "        # [2,4], [6,8,10] gives 10 corresponds to y[:2], split value = (2+3)/2 = 2.5, corresponding index = 2 which is value = 3\n",
        "        # [2,4,6], [8,10] gives 10 corresponds to y[:3], split value = (3+4)/2 = 3.5, corresponding index = 3 which is value = 4\n",
        "        # [2,4,6,8], [10] gives 10 corresponds to y[:4], split value = (4+5)/2 = 4.5\n",
        "        # [2,4,6,8,10], [] gives 40 corresponds to y[:5], split value = 5 + 5/2 = 7.5\n",
        "\n",
        "        # k = 0, 1, 2, 3, 4\n",
        "\n",
        "        # Best_split_value = (Sorted_X[Best_cutoff - 1] + Sorted_X[Best_cutoff]) / 2\n",
        "\n",
        "\n",
        "        # Base Case is so as I think there is no valid split if we are only down to 1 value for X, had to shift this to the top in order for it to work as intended\n",
        "        length = len(X) # len(X) should always be the same as len(y)\n",
        "        if depth == self.max_depth or length <= 1:\n",
        "          if length != 0:\n",
        "            mean_y = sum(y) / len(y)\n",
        "            return(mean_y)\n",
        "          else:\n",
        "            return(0)\n",
        "\n",
        "        # Need to consider if X is sorted in ascending order else slicing by index would not work, use sort() but not directly since we need to sort and keep corresponding X and y\n",
        "        # indexes the same so we use a list of tuples\n",
        "\n",
        "        list_of_tuples = []\n",
        "        for i in range(length):\n",
        "          list_of_tuples.append((X[i], y[i]))\n",
        "\n",
        "        Sorted_list = sorted(list_of_tuples, key=lambda x: x[0])\n",
        "        Sorted_X = []\n",
        "        Sorted_y = []\n",
        "\n",
        "        for j in range(length):\n",
        "          Sorted_X.append(Sorted_list[j][0])\n",
        "          Sorted_y.append(Sorted_list[j][1])\n",
        "\n",
        "        # Then we find the best cutoff by taking the smallest RSS\n",
        "        Best_RSS = 100000\n",
        "        Best_cutoff = -1\n",
        "\n",
        "        for k in range(1, length):\n",
        "          y_left = Sorted_y[:k]\n",
        "          y_right = Sorted_y[k:]\n",
        "          RSS = calculate_regionrss(y_left, y_right)\n",
        "\n",
        "          if RSS < Best_RSS: # Not <= for the split_value calculation later\n",
        "            Best_RSS = RSS\n",
        "            Best_cutoff = k\n",
        "\n",
        "        # Since we looped from 1 to length, we need to consider for cases outside that such as if it is a leaf node. If so then again just return mean_y again\n",
        "        if Best_cutoff == -1:\n",
        "          mean_y = sum(y) / len(y)\n",
        "          return(mean_y)\n",
        "\n",
        "        # Keep track for the left and right subtrees (outside the loop)\n",
        "        X_left = Sorted_X[:Best_cutoff]\n",
        "        X_right = Sorted_X[Best_cutoff:]\n",
        "        y_left = Sorted_y[:Best_cutoff]\n",
        "        y_right = Sorted_y[Best_cutoff:]\n",
        "\n",
        "        # We calculate split value by taking the midpoint, since it seems to be the best method\n",
        "        Best_split_value = (Sorted_X[Best_cutoff - 1] + Sorted_X[Best_cutoff]) / 2\n",
        "\n",
        "        left_subtree = self._build_tree(X_left, y_left, depth + 1)\n",
        "        right_subtree = self._build_tree(X_right, y_right, depth + 1)\n",
        "\n",
        "        res = {'split_value': Best_split_value, 'left': left_subtree, 'right': right_subtree}\n",
        "\n",
        "        \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "        return res\n",
        "\n",
        "    def predict_one(self, x, node=None):\n",
        "        \"\"\"\n",
        "        TODO: Traverse the tree to make a prediction for a single input.\n",
        "        Hint: Prediction should go to the right child node when it is exactly the same as the split value.\n",
        "\n",
        "        Args:\n",
        "            x: A single input feature value.\n",
        "            node: The current node in the tree (used for recursion)\n",
        "\n",
        "        Returns:\n",
        "            A float representing the predicted target value for the input\n",
        "        \"\"\"\n",
        "\n",
        "        prediction = 0\n",
        "\n",
        "        \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "        # Assume node = {'split_value': 3, 'left': {'split_value': 2, 'left': 2.0, 'right': 4.0}, 'right': {'split_value': 4, 'left': 6.0, 'right': 9.0}} from earlier print test case\n",
        "        # x = 4, so we compare x vs split_value so 4 >= 3, as per hint it should go to the right child node else left\n",
        "\n",
        "        if isinstance(node, float):\n",
        "          # We are at the leaf node so it is just the float value\n",
        "          prediction = node\n",
        "        else:\n",
        "          # We are still traversing, we have node as a dictionary\n",
        "          split_value = node.get('split_value', 0)\n",
        "          if x >= split_value:\n",
        "            right_child_node = node.get('right', None)\n",
        "            prediction = self.predict_one(x, node = right_child_node)\n",
        "\n",
        "          else:\n",
        "            left_child_node = node.get('left', None)\n",
        "            prediction = self.predict_one(x, node = left_child_node)\n",
        "\n",
        "        \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Call predict_one for each input in X and return the predictions.\n",
        "\n",
        "        Args:\n",
        "            X: A list of input feature values (one feature per sample)\n",
        "\n",
        "        Returns:\n",
        "            A list of predicted target values\n",
        "        \"\"\"\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "        res = 0\n",
        "\n",
        "        for i in X:\n",
        "          res = self.predict_one(i, node = self.tree)\n",
        "          predictions.append(res)\n",
        "\n",
        "        # print(predictions)\n",
        "\n",
        "        \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# TESTCASES 1.2\n",
        "import math\n",
        "\n",
        "X = [1, 2, 3, 4, 5]\n",
        "y = [2, 4, 6, 8, 10]\n",
        "\n",
        "model = DTRegressor(max_depth=2)\n",
        "model.fit(X, y)\n",
        "predictions = model.predict([1.5, 3.5, 5])\n",
        "assert all(isinstance(p, float) for p in predictions)\n",
        "assert len(predictions) == 3\n",
        "assert math.isclose(predictions[0], 4.0)\n",
        "assert math.isclose(predictions[1], 9.0)\n",
        "assert math.isclose(predictions[2], 9.0)\n",
        "\n",
        "\n",
        "X_offset = [0, 1, 2, 3]\n",
        "y_offset = [5, 7, 9, 11]\n",
        "model_offset = DTRegressor(max_depth=2)\n",
        "model_offset.fit(X_offset, y_offset)\n",
        "predictions_offset = model_offset.predict([0.5, 2.5])\n",
        "assert all(isinstance(p, float) for p in predictions_offset)\n",
        "assert len(predictions_offset) == 2\n",
        "assert math.isclose(predictions_offset[0], 7.0)\n",
        "assert math.isclose(predictions_offset[1], 11.0)\n",
        "\n",
        "print('All test cases passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4ae4dc",
      "metadata": {
        "id": "1c4ae4dc"
      },
      "source": [
        "## Task 2 - Decision Tree Classification [11 Points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f898a9d",
      "metadata": {
        "id": "6f898a9d"
      },
      "source": [
        "### Task 2.1 - Calculate Entropy [1 Point]\n",
        "\n",
        "The entropy of a label set quantifies the amount of uncertainty or impurity in the distribution of class labels. It is defined as:\n",
        "\n",
        "$$\n",
        "H(E) = -\\sum_{e\\in E}{P\\left(e\\right)\\log_{|E|}\\left(P\\left(e\\right)\\right)}\n",
        "$$\n",
        "\n",
        "where $P(e)$ is the proportion of samples belonging to class $e$, and $|E|$ is the number of unique classes in the label set. Implement `compute_entropy` using the `math` package.\n",
        "\n",
        "**Note**: Make sure that your code is able to handle the case where there is only 1 unique class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bd37bb57",
      "metadata": {
        "id": "bd37bb57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f2f6aa-fde3-40f9-f32c-f273477f5e37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ],
      "source": [
        "# TASK 2.1\n",
        "import math\n",
        "\n",
        "def compute_entropy(y, n_unique_classes):\n",
        "    \"\"\"\n",
        "    TODO: Compute class proportions and entropy using the formula.\n",
        "\n",
        "    Args:\n",
        "        y: A list of class labels (e.g., ['yes', 'no', 'yes', ...])\n",
        "        n_unique_classes: The number of unique classes in the dataset\n",
        "\n",
        "    Returns:\n",
        "        A float representing the entropy of the label distribution\n",
        "    \"\"\"\n",
        "\n",
        "    entropy = 0\n",
        "\n",
        "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "    # first consider for each type in n_unique_classes\n",
        "    if n_unique_classes == 1:\n",
        "      return(0)\n",
        "\n",
        "    else:\n",
        "      total = len(y)\n",
        "      unique_values = {}\n",
        "\n",
        "      for i in y:\n",
        "        # Find the unique values in y\n",
        "        unique_values[i] = unique_values.get(i,0) + 1\n",
        "\n",
        "      unique_value = list(unique_values.keys()) # yes, no\n",
        "\n",
        "      # Apply formula now\n",
        "      for j in unique_value:\n",
        "        P_e = unique_values.get(j, 0) / total\n",
        "        entropy += P_e * (math.log(P_e, n_unique_classes))\n",
        "\n",
        "      entropy = entropy * (-1)\n",
        "\n",
        "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "    return entropy\n",
        "\n",
        "# TESTCASES 2.1\n",
        "assert math.isclose(compute_entropy(['yes', 'no', 'yes', 'yes', 'no'], n_unique_classes=2), 0.970950, rel_tol=1e-5)\n",
        "assert math.isclose(compute_entropy(['yes', 'yes', 'yes','no','maybe','maybe','no','maybe'], n_unique_classes= 3), 0.985056, rel_tol=1e-5)\n",
        "assert math.isclose(compute_entropy(['cat', 'dog', 'cat', 'fish', 'dog', 'cat'], n_unique_classes= 3), 0.920619, rel_tol=1e-5)\n",
        "assert math.isclose(compute_entropy(['yes', 'yes', 'yes'], n_unique_classes=1), 0, rel_tol=1e-5)\n",
        "\n",
        "print('All test cases passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e06da2b9",
      "metadata": {
        "id": "e06da2b9"
      },
      "source": [
        "### Task 2.2 - Compute Information Gain [1 Point]\n",
        "\n",
        "The information gain of a split of the attribute $A$ is:\n",
        "\n",
        "$$\n",
        "\\operatorname{IG}(D,A)=H(D) - \\sum_{v\\in A} {\\frac{|D_v|}{|D|}}H(D_v)=H(D) - \\sum_{v\\in A} P(D_v)H(D_v)\n",
        "$$\n",
        "\n",
        "Implement `information_gain` using the function `compute_entropy` defined before.\n",
        "\n",
        "**Note**: Make sure that your code is able to handle the case where `parent_y` is an empty list. However, you may assume that each of the list in `list_of_child_ys` is a valid mutually-exclusive split of `parent_y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a0ad3dd7",
      "metadata": {
        "id": "a0ad3dd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "662305ef-3207-4019-b5f8-49b35536afc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ],
      "source": [
        "# TASK 2.2\n",
        "def information_gain(parent_y, list_of_child_ys, n_unique_classes=2):\n",
        "    \"\"\"\n",
        "    TODO: Compute the information gain from the parent to the children\n",
        "\n",
        "    Args:\n",
        "        parent_y: Target values of the parent node.\n",
        "        list_of_child_ys: A list where each element is a list of target values for a child node resulting from a split.\n",
        "        n_unique_classes: The number of unique classes in the dataset\n",
        "\n",
        "    Returns:\n",
        "        A float representing the information gain from the split\n",
        "    \"\"\"\n",
        "\n",
        "    gain = 0\n",
        "\n",
        "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "    # IG = Entropy(parent_y) - summation of sublist in (list_of_child_ys) of (proportion of sublist over parent * entropy(sublist))\n",
        "\n",
        "    total = len(parent_y)\n",
        "    if total == 0: # Need to ensure case where parent_y is [] but each list in list_of_child_ys is mutually exclusive from parent_y\n",
        "      gain = 0\n",
        "\n",
        "    else:\n",
        "      H_D = compute_entropy(parent_y, n_unique_classes)\n",
        "\n",
        "      for i in list_of_child_ys:\n",
        "        gain += (len(i) / total) * compute_entropy(i, n_unique_classes)\n",
        "\n",
        "      gain = (gain * -1) + H_D\n",
        "\n",
        "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "    return gain\n",
        "\n",
        "# TESTCASES 2.2\n",
        "import math\n",
        "\n",
        "parent = ['yes', 'no', 'yes', 'no']\n",
        "left = ['yes', 'yes']\n",
        "right = ['no', 'no']\n",
        "assert math.isclose(information_gain(parent, [left, right], n_unique_classes = 2), 1.0, rel_tol=1e-5)\n",
        "\n",
        "parent = ['yes', 'no', 'yes', 'no']\n",
        "left = ['yes', 'no']\n",
        "right = ['yes', 'no']\n",
        "assert math.isclose(information_gain(parent, [left, right], n_unique_classes = 2), 0.0, rel_tol=1e-5)\n",
        "\n",
        "\n",
        "parent = ['yes', 'no', 'yes', 'no', 'yes', 'no']\n",
        "child_1 = ['yes', 'yes']\n",
        "child_2 = ['no', 'no']\n",
        "child_3 = ['yes', 'no']\n",
        "assert math.isclose(information_gain(parent, [child_1, child_2, child_3], n_unique_classes=2), 2/3, rel_tol=1e-5)\n",
        "\n",
        "print('All test cases passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9968096",
      "metadata": {
        "id": "f9968096"
      },
      "source": [
        "### Task 2.3 Implement the Majority Class function [1 Point]\n",
        "\n",
        "This function finds the most frequent class label within a list of `y_labels`. When a new data point reaches a leaf node in a classification tree, it's assigned the class that's most common among the training examples in that leaf. This function determines that \"majority class.\" Count the occurrences of each label. If there's a tie for the highest count, return the smallest label (alphabetically first) to ensure consistent results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac2cb1c8",
      "metadata": {
        "id": "ac2cb1c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ec074d-166e-4bd5-c1f6-15a46310d05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ],
      "source": [
        "# TASK 2.3\n",
        "def majority_class(y_labels):\n",
        "        \"\"\"\n",
        "        TODO: Implement majority class calculation.\n",
        "\n",
        "        Args:\n",
        "            y: A list of class labels\n",
        "\n",
        "        Returns:\n",
        "            The majority class label (the one with the highest count), or None if y_labels is empty\n",
        "            If there is a tie, return the smallest label\n",
        "        \"\"\"\n",
        "\n",
        "        majority_class = None\n",
        "\n",
        "        \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "        count = {}\n",
        "        res = []\n",
        "\n",
        "        # Use a dictionary to count the number of occurences\n",
        "        for i in y_labels:\n",
        "          count[i] = count.get(i,0) + 1\n",
        "        # print(count)\n",
        "\n",
        "        # Convert to list then sort by value then alphabetically\n",
        "        res = list(count.items())\n",
        "\n",
        "        # res = sorted(res, key = lambda x: (x[1], x[0]), reverse = True) # Failed\n",
        "        # Need it to sort by the first element of the tuple in descending order, then second element of the tuple in ascending order\n",
        "        # first element is an int, second element is a str\n",
        "\n",
        "        res = sorted(res, key = lambda x: (-x[1], x[0])) # Trick is since when we multiply by -1, the ascending order becomes -3 then -2 which is what we want\n",
        "        # print(res)\n",
        "\n",
        "        majority_class = res[0][0]\n",
        "\n",
        "        \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "        return majority_class\n",
        "\n",
        "# TESTCASES 2.3\n",
        "y = ['yes', 'no', 'yes', 'no', 'yes']\n",
        "assert majority_class(y) == 'yes'\n",
        "\n",
        "y = ['yes', 'no', 'yes', 'no']\n",
        "assert majority_class(y) == 'no'\n",
        "\n",
        "y = [1, 2, 2, 3, 2]\n",
        "assert majority_class(y) == 2\n",
        "\n",
        "print('All test cases passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d590c17b",
      "metadata": {
        "id": "d590c17b"
      },
      "source": [
        "### Task 2.4 - Implement the Best Split function [3 Points]\n",
        "\n",
        "Complete the `find_best_split` function below using the `information_gain` function defined before.\n",
        "\n",
        "For a given dataset `X` (features) and `y` (labels), it searches through all possible ways to split the data based on each feature and selects the best one A decision tree grows by finding splits that best separate the classes. This function chooses the split with the highest Information Gain (IG), leading to purer child nodes and better classification..\n",
        "\n",
        "This function must handle both categorical and continuous features:\n",
        "* Categorical Features\n",
        "    * String Values like `'red'`, `'blue'`, `'apple'`, etc.\n",
        "    * Perform a **multi-way split**, creating one child node per unique category (at least 2 categories).\n",
        "    * DO NOT remove the chosen feature from X after splitting.\n",
        "\n",
        "* Continuous Features\n",
        "    * Numeric values like `2.0`, `4.5`, etc.\n",
        "    * Perform **binary splits** at midpoints between consecutive sorted unique values.\n",
        "\n",
        "For each potential split:\n",
        "1. Split the data accordingly.\n",
        "2. Calculate the information gain from the split.\n",
        "3. Track the split details (feature index, split type, and child group values).\n",
        "\n",
        "Return the split with the highest information gain. If there is a tie in information gain, prefer the feature with the lowest index.\n",
        "\n",
        "As this is quite a complex problem, we suggest that you cross-check your outputs manually and add more test cases to make sure that your code really works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f31b30d0",
      "metadata": {
        "id": "f31b30d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae4e169-eeb8-45aa-f986-98e4f01ffcce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ],
      "source": [
        "# TASK 2.4\n",
        "def find_best_split(X, y, n_unique_classes=2):\n",
        "    \"\"\"\n",
        "    TODO: Implement the best split finding logic for both categorical and continuous features\n",
        "\n",
        "    Args:\n",
        "        X: List of feature vectors\n",
        "        y: List of target labels.\n",
        "        n_unique_classes: The total number of unique classes in the dataset, used as log base.\n",
        "\n",
        "    Returns:\n",
        "        A tuple:\n",
        "        (\n",
        "            best_gain,             # float: Highest information gain achieved by any split\n",
        "            best_feature_idx,      # int: Index of the feature that gives the best split\n",
        "            best_split_type,       # str: Either 'categorical' or 'continuous'\n",
        "            best_split_details     # dict: Structure varies based on split type (see below)\n",
        "        )\n",
        "\n",
        "        - For a categorical feature, best_split_details should be:\n",
        "            {\n",
        "                category_val_1: {'X': [...], 'y': [...]},\n",
        "                category_val_2: {'X': [...], 'y': [...]},\n",
        "                ...\n",
        "            }\n",
        "\n",
        "        - For a continuous feature, best_split_details should be:\n",
        "            {\n",
        "                'split_value': val,        # float: midpoint value used for binary split\n",
        "                'left_X': [...], 'left_y': [...],\n",
        "                'right_X': [...], 'right_y': [...]\n",
        "            }\n",
        "\n",
        "        If no valid split is found, return:\n",
        "            (-1.0, None, None, None)\n",
        "    \"\"\"\n",
        "\n",
        "    best_overall_gain = -1.0\n",
        "    best_overall_feature_index = None\n",
        "    best_overall_split_type = None\n",
        "    best_overall_split_details = None\n",
        "\n",
        "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "    # Based on task description, it should work on multiple X categorical variables so including X can be [['red', 1.0], ['blue', 2.0], ['red', 3.0], ...]\n",
        "    length = len(X)\n",
        "    # best_overall_gain should remain at -1.0 if there is really no valid split so if X is []\n",
        "    if length == 0:\n",
        "      return best_overall_gain, best_overall_feature_index, best_overall_split_type, best_overall_split_details\n",
        "\n",
        "    else: # I don't think i need to put the rest of the code inside the else, but i will do so just to be safe\n",
        "      number_of_category = len(X[0])\n",
        "\n",
        "      for feature_index in range(number_of_category):\n",
        "        current_split_type = None # Needed since i get unbound error without it\n",
        "\n",
        "        if isinstance(X[0][feature_index], str):\n",
        "          # Categorical data\n",
        "          current_split_type = \"categorical\"\n",
        "\n",
        "          # Next we find the number of unique categories\n",
        "          category_values = {}\n",
        "          unique_categories = []\n",
        "\n",
        "          for i in range(length):\n",
        "            category_value = X[i][feature_index]\n",
        "\n",
        "            if category_value not in unique_categories:\n",
        "              unique_categories.append(category_value)\n",
        "              category_values[category_value] = {'X': [], 'y': []}\n",
        "            else:\n",
        "              pass\n",
        "\n",
        "            category_values[category_value]['X'].append(X[i])\n",
        "            category_values[category_value]['y'].append(y[i])\n",
        "\n",
        "          # print(unique_categories) # ['red', 'blue', 'green']\n",
        "          # print(category_values) # {'red': {'X': [['red'], ['red']], 'y': ['yes', 'yes']}, 'blue': {'X': [['blue'], ['blue']], 'y': ['no', 'no']}, 'green': {'X': [['green']], 'y': ['no']}}\n",
        "          # Is this the multi-split in \"Perform a multi-way split, creating one child node per unique category (at least 2 categories).\"\n",
        "\n",
        "          # Create list_of_child_ys\n",
        "          list_of_child_ys = []\n",
        "\n",
        "          for j in unique_categories:\n",
        "            category_group_dict = category_values.get(j, None) # Also a dictionary, {'X': [['red'], ['red']], 'y': ['yes', 'yes']}\n",
        "            category_group_values = category_group_dict.get('y', None) # Should be a list, ['yes', 'yes']\n",
        "            list_of_child_ys.append(category_group_values)\n",
        "\n",
        "          # Implement multi-split??? since there is some best_overall_feature_index, i think it means that X can be [['red', 1.0], ['blue', 2.0], ['red', 3.0], ...]\n",
        "          IG = information_gain(y, list_of_child_ys, n_unique_classes)\n",
        "\n",
        "          if IG > best_overall_gain and IG > 0.0: # If IG is not positive then the split is not valid/meaningful hence we let best_overall_gain = -1.0 still\n",
        "            best_overall_gain = IG\n",
        "            best_overall_split_details = category_values\n",
        "            best_overall_feature_index = feature_index\n",
        "            best_overall_split_type = current_split_type\n",
        "\n",
        "          elif IG == best_overall_gain and feature_index < best_overall_feature_index and IG > 0.0: # If IG is not positive then the split is not valid/meaningful hence we let best_overall_gain = -1.0 still:\n",
        "            # If there is a tie in information gain, prefer the feature with the lowest index.\n",
        "            best_overall_gain = IG\n",
        "            best_overall_split_details = category_values\n",
        "            best_overall_feature_index = feature_index\n",
        "            best_overall_split_type = current_split_type\n",
        "\n",
        "          else:\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "          # Continuous data, structure is similar to for categorical\n",
        "          current_split_type = \"continuous\"\n",
        "\n",
        "          list_of_tuples = []\n",
        "          for k in range(length):\n",
        "            list_of_tuples.append((X[k][feature_index], y[k], X[k])) # X = [['red', 1.0], ['blue', 2.0], ['red', 3.0], ...]\n",
        "\n",
        "          Sorted_list = sorted(list_of_tuples, key=lambda x: x[0])\n",
        "          # print(Sorted_list)\n",
        "\n",
        "          # Original method, failed at 2.6 since we needed to keep the list of list structure so modifed from tuple of 2 to 3\n",
        "          # Sorted_X = []\n",
        "          # Sorted_y = []\n",
        "\n",
        "          # for l in range(length):\n",
        "          #   Sorted_X.append(Sorted_list[l][0])\n",
        "          #   Sorted_y.append(Sorted_list[l][1])\n",
        "\n",
        "          # print(Sorted_X) # [2.0, 4.0, 6.0, 8.0, 10.0]\n",
        "\n",
        "          # Find midpoints for splitting\n",
        "          # mid_points = []\n",
        "\n",
        "          for u in range(length - 1): # length = len(X)\n",
        "            # We consider if there are duplicated values in Sorted_X if so we don't create a midpoint\n",
        "            if Sorted_list[u][0] != Sorted_list[u+1][0]:\n",
        "              mid_point = (Sorted_list[u][0] + Sorted_list[u+1][0]) / 2\n",
        "              # mid_points.append(mid_point) # Not needed since we can handle all in a loop\n",
        "\n",
        "              # Binary split by mid_point\n",
        "              left_y = []\n",
        "              right_y = []\n",
        "              left_X = []\n",
        "              right_X = []\n",
        "              for r in Sorted_list:\n",
        "                # print(Sorted_X) # Again legacy code\n",
        "                # print(r)\n",
        "                X_value = r[0]\n",
        "                y_value = r[1]\n",
        "                X_list_form = r[2]\n",
        "\n",
        "                if X_value < mid_point: # Recall from lecture 3a, for our course we use < for the left subtree\n",
        "                # Had to changed from using sorted list to original X and y since in task 2.6 it was not following the format of a list of list\n",
        "                  left_y.append(y_value) # left_y.append(Sorted_y[r])\n",
        "                  left_X.append(X_list_form) # left_X.append(Sorted_X[r])\n",
        "                else:\n",
        "                  right_y.append(y_value) # right_y.append(Sorted_y[r])\n",
        "                  right_X.append(X_list_form) # right_X.append(Sorted_X[r]\n",
        "\n",
        "              # Now we evaluate based on IG\n",
        "              list_of_child_ys = [left_y, right_y]\n",
        "              IG = information_gain(y, list_of_child_ys, n_unique_classes)\n",
        "\n",
        "              # Almost the same as continuous except for best_overall_split_details\n",
        "              if IG > best_overall_gain and IG > 0.0: # If IG is not positive then the split is not valid/meaningful hence we let best_overall_gain = -1.0 still\n",
        "                best_overall_gain = IG\n",
        "                best_overall_feature_index = feature_index\n",
        "                best_overall_split_type = current_split_type\n",
        "                best_overall_split_details = {'split_value':mid_point, 'left_X':left_X, 'left_y':left_y, 'right_X':right_X, 'right_y':right_y}\n",
        "\n",
        "                # {\n",
        "                #   'split_value': val,        # float: midpoint value used for binary split\n",
        "                #   'left_X': [...], 'left_y': [...],\n",
        "                #   'right_X': [...], 'right_y': [...]\n",
        "                # }\n",
        "              elif IG == best_overall_gain and feature_index < best_overall_feature_index and IG > 0.0: # If IG is not positive then the split is not valid/meaningful hence we let best_overall_gain = -1.0 still\n",
        "              # If there is a tie in information gain, prefer the feature with the lowest index.\n",
        "                best_overall_gain = IG\n",
        "                best_overall_feature_index = feature_index\n",
        "                best_overall_split_type = current_split_type\n",
        "                best_overall_split_details = {'split_value':mid_point, 'left_X':left_X, 'left_y':left_y, 'right_X':right_X, 'right_y':right_y}\n",
        "\n",
        "              else:\n",
        "                pass\n",
        "\n",
        "            else:\n",
        "              pass\n",
        "\n",
        "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "    return best_overall_gain, best_overall_feature_index, best_overall_split_type, best_overall_split_details\n",
        "\n",
        "# TESTCASES 2.4\n",
        "import math\n",
        "\n",
        "# TEST 1: Categorical Feature Split\n",
        "X = [['red'], ['blue'], ['red'], ['green'], ['blue']]\n",
        "y = ['yes', 'no', 'yes', 'no', 'no']\n",
        "n_unique_classes = 2\n",
        "gain, feature_idx, split_type, split_details = find_best_split(X, y, n_unique_classes)\n",
        "\n",
        "assert split_type == 'categorical'\n",
        "assert feature_idx == 0\n",
        "assert math.isclose(gain, 0.97095, rel_tol=1e-4)\n",
        "assert isinstance(split_details, dict)\n",
        "assert set(split_details.keys()) == {'red', 'blue', 'green'}\n",
        "\n",
        "# TEST 2: Continuous Feature Split\n",
        "X = [[2.0], [4.0], [6.0], [8.0], [10.0]]\n",
        "y = ['yes', 'yes', 'no', 'no', 'no']\n",
        "n_unique_classes = 2\n",
        "gain, feature_idx, split_type, split_details = find_best_split(X, y, n_unique_classes)\n",
        "\n",
        "assert split_type == 'continuous'\n",
        "assert feature_idx == 0\n",
        "assert isinstance(split_details, dict)\n",
        "assert 'split_value' in split_details\n",
        "assert split_details['split_value'] == 5.0  # Midpoint between 4.0 and 6.0\n",
        "assert math.isclose(gain, 0.97095, rel_tol=1e-4)\n",
        "\n",
        "# TEST 3: No Valid Split\n",
        "X = [['same'], ['same'], ['same']]\n",
        "y = ['yes', 'yes', 'yes']\n",
        "n_unique_classes = 1\n",
        "gain, feature_idx, split_type, split_details = find_best_split(X, y, n_unique_classes)\n",
        "\n",
        "assert gain == -1.0\n",
        "assert feature_idx is None\n",
        "assert split_type is None\n",
        "assert split_details is None\n",
        "\n",
        "print('All test cases passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54923c12",
      "metadata": {
        "id": "54923c12"
      },
      "source": [
        "### Task 2.5 - Implement the Recursive Tree Builder Function [2 Points]\n",
        "\n",
        "Complete the function `build_tree_recursive` below using the `find_best_split` and `majority_class` functions defined before.\n",
        "\n",
        "It works recursively to grow the tree branch by branch. At each step, it decides whether to stop growing (i.e., return a leaf node) or to split the data and create internal nodes with children. Use `majority_class` to label leaf nodes and `find_best_split` to decide the best way to divide data at internal nodes.\n",
        "\n",
        "Base Cases: Stop Recursion When Any of These is True:\n",
        "\n",
        "- The maximum depth (`max_depth`) is reached.\n",
        "- There is no further valid split, according to `find_best_split`\n",
        "- All `y` labels in the current subset are the same class (pure node).\n",
        "\n",
        "In these cases, return the majority class of the current `y` subset as the leaf node's prediction.\n",
        "\n",
        "Recursive Step: If No Base Case is Triggered\n",
        "\n",
        "1. Call `find_best_split(X, y, n_unique_classes)` to determine:\n",
        "   - Best feature index\n",
        "   - Type of feature (`'continuous'` or `'categorical'`)\n",
        "   - Split details (values and data partitions)\n",
        "\n",
        "2. Construct a dictionary representing an internal node, which includes:\n",
        "   - `'feature'`: index of the best splitting feature.\n",
        "   - `'split_type'`: `'continuous'` or `'categorical'`.\n",
        "   - For continuous features:\n",
        "     - `'split_value'`: float value for binary split.\n",
        "     - `'left'` and `'right'`: recursive child nodes.\n",
        "   - For categorical features:\n",
        "     - `'children_map'`: a dictionary mapping each category to a recursive child node.\n",
        "     - `'default_prediction'`: the majority class at this node (used as fallback during inference)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ff5783",
      "metadata": {
        "id": "33ff5783",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc5eb36a-04f7-43b6-a064-d6f18443c622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Test Case 1: Max depth reached\n",
            "Test Case 1 Passed: Correctly returned leaf value 0.\n",
            "------------------------------\n",
            "Running Test Case 2: All labels same\n",
            "Test Case 2 Passed: Correctly returned leaf value 0.\n",
            "------------------------------\n",
            "Running Test Case 3: Empty X\n",
            "Test Case 3 Passed: Correctly returned leaf value 0.\n",
            "------------------------------\n",
            "All test cases passed!\n"
          ]
        }
      ],
      "source": [
        "# TASK 2.5\n",
        "def build_tree_recursive(X, y, depth, max_depth, n_unique_classes):\n",
        "    \"\"\"\n",
        "    TODO: Implement the recursive tree building logic using the best split found.\n",
        "\n",
        "    Args:\n",
        "        X: Current subset of feature vectors.\n",
        "        y: Current subset of target labels.\n",
        "        depth: Current depth of the tree.\n",
        "        max_depth: Maximum allowed depth for the tree.\n",
        "        n_unique_classes: The total number of unique classes in the dataset, used as log base.\n",
        "\n",
        "    Returns:\n",
        "        Either:\n",
        "            - A leaf node: the majority class label (int) if the tree should stop splitting.\n",
        "            - OR a decision node (dict) with the following structure:\n",
        "\n",
        "        For continuous features:\n",
        "            {\n",
        "                'feature': <feature_index>,                # int, index of feature used for split\n",
        "                'split_type': 'continuous',               # str\n",
        "                'split_value': <threshold>,               # float, the midpoint used for binary split\n",
        "                'left': <left_subtree>,                   # recursive subtree or leaf\n",
        "                'right': <right_subtree>,                 # recursive subtree or leaf\n",
        "                'default_prediction': <majority_class>    # int, used for fallback predictions\n",
        "            }\n",
        "\n",
        "        For categorical features:\n",
        "            {\n",
        "                'feature': <feature_index>,                # int\n",
        "                'split_type': 'categorical',              # str\n",
        "                'children_map': {\n",
        "                    <category_val>: <subtree_or_leaf>,    # one child per category\n",
        "                    ...\n",
        "                },\n",
        "                'default_prediction': <majority_class>    # int\n",
        "            }\n",
        "    \"\"\"\n",
        "\n",
        "    res = None\n",
        "\n",
        "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "    gain, feature_idx, split_type, split_details = find_best_split(X, y, n_unique_classes) # Copied the testcase from find_best_split in task 2.4\n",
        "    # if gain == -1 then the there should be no valid split\n",
        "    unique_categories = dict.fromkeys(y, 0)\n",
        "    stop = len(unique_categories) # len(set(y)) which does the same thing also works\n",
        "    maj = majority_class(y)\n",
        "\n",
        "    # Base case\n",
        "    if depth == max_depth or gain == -1 or stop == 1:\n",
        "      return(maj)\n",
        "\n",
        "    else:\n",
        "      # Construct a dictionary representing an internal node, which includes:\n",
        "      # 'feature': index of the best splitting feature.\n",
        "      # 'split_type': 'continuous' or 'categorical'.\n",
        "      # For continuous features:\n",
        "      # 'split_value': float value for binary split.\n",
        "      # 'left' and 'right': recursive child nodes.\n",
        "      # For categorical features:\n",
        "      # 'children_map': a dictionary mapping each category to a recursive child node.\n",
        "      # 'default_prediction': the majority class at this node (used as fallback during inference).\n",
        "\n",
        "      # Most basic structure\n",
        "      internal_node = {'feature':feature_idx, 'split_type':split_type, 'default_prediction':maj}\n",
        "\n",
        "      if split_type == 'categorical':\n",
        "        # Recall split_details = {'red': {'X': [['red'], ['red']], 'y': ['yes', 'yes']}, 'blue': {'X': [['blue'], ['blue']], 'y': ['no', 'no']}, 'green': {'X': [['green']], 'y': ['no']}}\n",
        "        categories = split_details.keys()\n",
        "        # We create the 'children_map' dictionary\n",
        "        internal_node['children_map'] = {}\n",
        "\n",
        "        for i in categories:\n",
        "          dic = split_details[i] # dic is just the dict of [['red'], ['red']] and so on\n",
        "          dic_X = dic['X']\n",
        "          dic_y = dic['y']\n",
        "          internal_node['children_map'][i] = build_tree_recursive(dic_X, dic_y, depth + 1, max_depth, n_unique_classes)\n",
        "        return(internal_node)\n",
        "\n",
        "      elif split_type == 'continuous':\n",
        "        split_value = split_details['split_value']\n",
        "        left_X = split_details['left_X']\n",
        "        right_X = split_details['right_X']\n",
        "        left_y = split_details['left_y']\n",
        "        right_y = split_details['right_y']\n",
        "\n",
        "        internal_node['split_value'] = split_value\n",
        "\n",
        "        # Swap the assignments to match the prediction logic\n",
        "        # It appears that the data for the left branch is in right_X/right_y\n",
        "        # and vice versa.\n",
        "        internal_node['left'] = build_tree_recursive(left_X, left_y, depth + 1, max_depth, n_unique_classes)\n",
        "        internal_node['right'] = build_tree_recursive(right_X, right_y, depth + 1, max_depth, n_unique_classes)\n",
        "\n",
        "        return internal_node\n",
        "\n",
        "      else: # split_type == None\n",
        "        pass\n",
        "\n",
        "      # Added edge case of X = [] for task 2.4\n",
        "\n",
        "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "    return res\n",
        "\n",
        "# TESTCASES 2.5\n",
        "\n",
        "test_cases = [\n",
        "    # Test Case 1: Base Case - Max depth reached\n",
        "    {\n",
        "        \"name\": \"Max depth reached\",\n",
        "        \"X\": [[1], [2], [3]],\n",
        "        \"y\": [0, 1, 0],\n",
        "        \"depth\": 2,\n",
        "        \"max_depth\": 2,\n",
        "        \"n_unique_classes\": 2,\n",
        "        \"expected\": 0 # Majority class of [0, 1, 0] is 0\n",
        "    },\n",
        "    # Test Case 2: Base Case - All labels are the same\n",
        "    {\n",
        "        \"name\": \"All labels same\",\n",
        "        \"X\": [[1], [2], [3]],\n",
        "        \"y\": [0, 0, 0],\n",
        "        \"depth\": 0,\n",
        "        \"max_depth\": 5,\n",
        "        \"n_unique_classes\": 2,\n",
        "        \"expected\": 0 # Majority class of [0, 0, 0] is 0\n",
        "    },\n",
        "    # Test Case 3: Base Case - Empty X (should return majority of y)\n",
        "    {\n",
        "        \"name\": \"Empty X\",\n",
        "        \"X\": [],\n",
        "        \"y\": [0, 1, 0],\n",
        "        \"depth\": 0,\n",
        "        \"max_depth\": 5,\n",
        "        \"n_unique_classes\": 2,\n",
        "        \"expected\": 0 # Majority class of [0, 1, 0] is 0\n",
        "    }\n",
        "]\n",
        "\n",
        "# Helper function for running tests\n",
        "for i, tc in enumerate(test_cases):\n",
        "    print(f\"Running Test Case {i+1}: {tc['name']}\")\n",
        "    result = build_tree_recursive(tc['X'], tc['y'], tc['depth'], tc['max_depth'], tc['n_unique_classes'])\n",
        "\n",
        "    if \"expected_type\" in tc:\n",
        "        if tc[\"expected_type\"] == \"continuous_node\":\n",
        "            assert isinstance(result, dict) and result.get('split_type') == 'continuous', \\\n",
        "                f\"Test Case {i+1} Failed: Expected continuous node, got {result}\"\n",
        "            print(f\"Test Case {i+1} Passed: Correctly identified continuous node.\")\n",
        "        elif tc[\"expected_type\"] == \"categorical_node\":\n",
        "            assert isinstance(result, dict) and result.get('split_type') == 'categorical', \\\n",
        "                f\"Test Case {i+1} Failed: Expected categorical node, got {result}\"\n",
        "            print(f\"Test Case {i+1} Passed: Correctly identified categorical node.\")\n",
        "        elif tc[\"expected_type\"] == \"continuous_node_with_leaf_children\":\n",
        "            assert isinstance(result, dict) and result.get('split_type') == 'continuous', \\\n",
        "                f\"Test Case {i+1} Failed: Expected continuous node at root, got {result}\"\n",
        "            assert not isinstance(result['left'], dict) and not isinstance(result['right'], dict), \\\n",
        "                f\"Test Case {i+1} Failed: Expected leaf children, but found nodes. Left: {result['left']}, Right: {result['right']}\"\n",
        "            print(f\"Test Case {i+1} Passed: Correctly built continuous node with leaf children.\")\n",
        "    else:\n",
        "        assert result == tc['expected'], \\\n",
        "            f\"Test Case {i+1} Failed: Expected {tc['expected']}, got {result}\"\n",
        "        print(f\"Test Case {i+1} Passed: Correctly returned leaf value {result}.\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "print('All test cases passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a9d93e",
      "metadata": {
        "id": "d2a9d93e"
      },
      "source": [
        "### Task 2.6  - Implement Prediction Logic for a single data point [2 Points]\n",
        "\n",
        "Complete the `predict_one_instance` function below. It takes a single data point and \"walks\" it down the decision tree to make a prediction.\n",
        "\n",
        "Starting from the root node:\n",
        "- Recursively check the `split_type` (`'continuous'` or `'categorical'`) and the feature used at the current `tree_node`.\n",
        "- Based on `x_instance`'s value for that feature, decide:\n",
        "  - whether to go `left` or `right` (for continuous),\n",
        "  - or which `category` branch to follow (for categorical).\n",
        "\n",
        "Continue this process until a leaf node, which contains the final predicted class.\n",
        "\n",
        "Base Case:\n",
        "\n",
        "- If `tree_node` is not a dictionary, it's a leaf node.\n",
        "- Return its value directly (the predicted class).\n",
        "\n",
        "Traversal Logic:\n",
        "\n",
        "*   Continuous Split:\n",
        "    - Compare `x_instance[feature_idx]` with `tree_node['split_value']`.\n",
        "    - If less, recurse into `tree_node['left']`; else recurse into `tree_node['right']`.\n",
        "\n",
        "*   Categorical Split:\n",
        "    - Look up `x_instance[feature_idx]` in `tree_node['children_map']`.\n",
        "    - If found, recurse into the matching child.\n",
        "    - If unseen category, return `tree_node['default_prediction']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ff14d2",
      "metadata": {
        "id": "c7ff14d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b96ddf-02f4-4ee7-831f-a68bb7a21966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Case 1 Passed (build & continuous prediction left)\n",
            "Test Case 1 Passed (build & continuous prediction right)\n",
            "Test Case 2 Passed (build & categorical prediction 'apple')\n",
            "Test Case 2 Passed (build & categorical prediction 'grape' - default)\n",
            "Test Case 3 Passed (mixed features - continuous path)\n",
            "All test cases passed!\n"
          ]
        }
      ],
      "source": [
        "# TASK 2.6\n",
        "def predict_one_instance(x_instance, tree_node):\n",
        "    \"\"\"\n",
        "    TODO: Traverse the decision tree to make a prediction for a single input instance.\n",
        "\n",
        "    Args:\n",
        "        x_instance: A single feature vector (e.g., [val1, val2])\n",
        "        tree_node: The current node of the decision tree (starts with the root)\n",
        "\n",
        "    Returns:\n",
        "        The predicted class label\n",
        "    \"\"\"\n",
        "\n",
        "    predicted_label = None\n",
        "\n",
        "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "    # Base case\n",
        "    if not isinstance(tree_node, dict): # not allows mean to invert the False to True\n",
        "      return(tree_node) # Return its value directly (the predicted class).\n",
        "\n",
        "    else:\n",
        "      # Follow naming convention from earlier\n",
        "      split_type = tree_node['split_type']\n",
        "      feature_idx = tree_node['feature']\n",
        "      value = x_instance[feature_idx]\n",
        "\n",
        "      if split_type == 'categorical':\n",
        "        matching_child = tree_node['children_map'].get(value, tree_node['default_prediction'])\n",
        "\n",
        "        # We need to differentiate between the 2 possible matching_child so default_prediction is an int value based on earlier information\n",
        "        if isinstance(matching_child, int):\n",
        "          return(matching_child)\n",
        "\n",
        "        else: # If found, recurse into the matching child. So we call the function again but with matching_child\n",
        "          return(predict_one_instance(x_instance, matching_child))\n",
        "\n",
        "      elif split_type == 'continuous':\n",
        "        split_value = tree_node['split_value']\n",
        "\n",
        "        # Compare x_instance[feature_idx] with tree_node['split_value'].\n",
        "        if value < split_value:\n",
        "          # If less, recurse into tree_node['left']; else recurse into tree_node['right'].\n",
        "          return(predict_one_instance(x_instance, tree_node['left']))\n",
        "\n",
        "        else:\n",
        "          return(predict_one_instance(x_instance, tree_node['right']))\n",
        "\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# TESTCASES 2.6\n",
        "\n",
        "# Test Case 1: Build a simple tree with a continuous split and predict.\n",
        "X_tc1 = [[3.0], [7.0], [2.0], [8.0]]\n",
        "y_tc1 = [0, 1, 0, 1]\n",
        "n_unique_classes_tc1 = 2\n",
        "max_depth_tc1 = 1\n",
        "tree_tc1 = build_tree_recursive(X_tc1, y_tc1, 0, max_depth_tc1, n_unique_classes_tc1)\n",
        "# Predict an instance that goes left\n",
        "prediction_tc1_left = predict_one_instance([2.5], tree_tc1)\n",
        "assert prediction_tc1_left == 0, f\"Test Case 1 Failed (left prediction): Expected 0, got {prediction_tc1_left}\"\n",
        "print(\"Test Case 1 Passed (build & continuous prediction left)\")\n",
        "# Predict an instance that goes right\n",
        "prediction_tc1_right = predict_one_instance([6.0], tree_tc1)\n",
        "assert prediction_tc1_right == 1, f\"Test Case 1 Failed (right prediction): Expected 1, got {prediction_tc1_right}\"\n",
        "print(\"Test Case 1 Passed (build & continuous prediction right)\")\n",
        "\n",
        "\n",
        "# Test Case 2: Build a simple tree with a categorical split and predict.\n",
        "X_tc2 = [['apple'], ['banana'], ['apple'], ['orange']]\n",
        "y_tc2 = [0, 1, 0, 1]\n",
        "n_unique_classes_tc2 = 2\n",
        "max_depth_tc2 = 1\n",
        "tree_tc2 = build_tree_recursive(X_tc2, y_tc2, 0, max_depth_tc2, n_unique_classes_tc2)\n",
        "# Predict a known category ('apple')\n",
        "prediction_tc2_apple = predict_one_instance(['apple'], tree_tc2)\n",
        "assert prediction_tc2_apple == 0, f\"Test Case 2 Failed (apple prediction): Expected 0, got {prediction_tc2_apple}\"\n",
        "print(\"Test Case 2 Passed (build & categorical prediction 'apple')\")\n",
        "# Predict an unknown category ('grape') - should use default_prediction\n",
        "prediction_tc2_grape = predict_one_instance(['grape'], tree_tc2)\n",
        "assert prediction_tc2_grape == majority_class(y_tc2), f\"Test Case 2 Failed (grape prediction): Expected default {majority_class(y_tc2)}, got {prediction_tc2_grape}\"\n",
        "print(\"Test Case 2 Passed (build & categorical prediction 'grape' - default)\")\n",
        "\n",
        "# Test Case 3: Mixed features - build a tree, and predict a continuous feature value path.\n",
        "X_tc3 = [[10, 'A'], [2, 'B'], [12, 'A'], [3, 'B']]\n",
        "y_tc3 = [0, 1, 0, 1]\n",
        "n_unique_classes_tc3 = 2\n",
        "max_depth_tc3 = 1\n",
        "tree_tc3 = build_tree_recursive(X_tc3, y_tc3, 0, max_depth_tc3, n_unique_classes_tc3)\n",
        "# Predict an instance (continuous feature 0 value: 4, which is <= 5)\n",
        "prediction_tc3 = predict_one_instance([4, 'C'], tree_tc3) # The 'C' doesn't matter for this split\n",
        "assert prediction_tc3 == 1, f\"Test Case 3 Failed: Expected 1, got {prediction_tc3}\" # Majority of [1,1] from left branch if split at 5\n",
        "print(\"Test Case 3 Passed (mixed features - continuous path)\")\n",
        "\n",
        "print('All test cases passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27ba8c6b",
      "metadata": {
        "id": "27ba8c6b"
      },
      "source": [
        "### Task 2.7 - Decision Tree Classifier [1 Point]\n",
        "\n",
        "Complete the class below using the `majority_class`, `build_tree_recursive` and `predict_one_instance` functions defined before. We have already imported them for you on Coursemology.\n",
        "\n",
        "This class brings together all the previous helper functions to create, train, and use a decision tree for classification tasks.\n",
        "\n",
        "* `fit(self, X, y)`:\n",
        "    *   Calculate and store the total number of unique classes from `y` in `self.n_unique_classes`.\n",
        "    *   Otherwise, call `build_tree_recursive` to start building the tree, passing all necessary parameters including `self.max_depth` and `self.n_unique_classes`. Store the resulting tree structure in `self.tree`.\n",
        "    *   DO NOT return anything!\n",
        "\n",
        "*   `predict(self, X)`:\n",
        "    *   If `self.tree` is a leaf node (not a dictionary), simply return a list of that leaf value repeated for each instance in `X`.\n",
        "    *   Otherwise, iterate through each `x_instance` in the input `X` and call `predict_one_instance` to get a prediction for each. Collect these predictions into a list and return it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d24c495d",
      "metadata": {
        "id": "d24c495d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa8f28c-c986-4efa-bb10-ca7b5a34e409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ],
      "source": [
        "# TASK 2.7\n",
        "class DTClassifier:\n",
        "    def __init__(self, max_depth=2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_depth (int): The maximum depth of the tree.\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "        self.n_unique_classes = None # To store the total number of unique classes\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        TODO: Fit the decision tree to the training data. DO NOT return anything!\n",
        "\n",
        "        Args:\n",
        "            X: A list of feature vectors for training.\n",
        "            y: A list of corresponding target labels.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "        # Calculate and store the total number of unique classes from y in self.n_unique_classes\n",
        "        unique_categories = dict.fromkeys(y, 0)\n",
        "        total = len(unique_categories)\n",
        "        self.n_unique_classes = total\n",
        "\n",
        "        self.tree = build_tree_recursive(X, y, 0, self.max_depth, self.n_unique_classes)\n",
        "\n",
        "        \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Predict class labels for each instance in X using the fitted tree\n",
        "\n",
        "        Args:\n",
        "            X: A list of feature vectors for prediction.\n",
        "\n",
        "        Returns:\n",
        "            A list of predicted class labels.\n",
        "        \"\"\"\n",
        "\n",
        "        predicted_labels = []\n",
        "\n",
        "        \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "        # If self.tree is a leaf node (not a dictionary), simply return a list of that leaf value repeated for each instance in X.\n",
        "        if not isinstance(self.tree, dict):\n",
        "          length = len(X)\n",
        "\n",
        "          for i in range(length):\n",
        "            predicted_labels.append(self.tree)\n",
        "\n",
        "        # Otherwise, iterate through each x_instance in the input X and call predict_one_instance to get a prediction for each. Collect these predictions into a list and return it.\n",
        "        else:\n",
        "          for j in X:\n",
        "            res = predict_one_instance(j, self.tree)\n",
        "            predicted_labels.append(res)\n",
        "\n",
        "        \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "        return predicted_labels\n",
        "\n",
        "# TESTCASES 2.7\n",
        "\n",
        "# Test Case 1\n",
        "X = [[1.0], [2.0], [3.0]]\n",
        "y = [0, 0, 0]\n",
        "clf = DTClassifier(max_depth=5)\n",
        "clf.fit(X, y)\n",
        "X_predict = [[10.0], [20.0]]\n",
        "predictions = clf.predict(X_predict)\n",
        "assert predictions == [0,0] , f\"Prediction for a single-leaf tree should return that leaf's value for all instances.\"\n",
        "\n",
        "# Test Case 2\n",
        "X_train = [[3.0], [7.0], [2.0], [8.0]]\n",
        "y_train = [0, 1, 0, 1]\n",
        "max_depth = 1\n",
        "clf = DTClassifier(max_depth=max_depth)\n",
        "clf.fit(X_train, y_train)\n",
        "X_predict = [[2.5], [6.0], [4.9], [5.1]]\n",
        "predictions = clf.predict(X_predict)\n",
        "assert predictions == [0, 1, 0, 1] , f\"Prediction for continuous features should correctly traverse the tree.\"\n",
        "\n",
        "# Test Case 3\n",
        "X_train = [['red'], ['blue'], ['red'], ['green']]\n",
        "y_train = [0, 1, 0, 1]\n",
        "max_depth = 1\n",
        "clf = DTClassifier(max_depth=max_depth)\n",
        "clf.fit(X_train, y_train)\n",
        "X_predict = [['red'], ['blue'], ['yellow']]\n",
        "predictions = clf.predict(X_predict)\n",
        "assert predictions == [0, 1, majority_class(y_train)] , f\"Prediction for categorical features should correctly use children_map and handle unseen categories.\"\n",
        "\n",
        "print('All test cases passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5cd5f4b",
      "metadata": {
        "id": "a5cd5f4b"
      },
      "source": [
        "## Task 3 - Practical [2 Points]\n",
        "\n",
        "Train a DT classifier on the training dataset using `scikit-learn` and tune its hyperparameters to optimize performance.\n",
        "\n",
        "You will get full marks if your modelling is appropriate and performs well. But remember, you **MUST NOT** use or access X_test and y_test in your code, as this defeats the purpose of a hidden test set. Any model that does so will be given 0 mark.\n",
        "\n",
        "Make sure that you have installed `scikit-learn` in your python environment.\n",
        "\n",
        "**HINT**: Set the `random_state` parameter (if exists) to a certain constant to make your model reproducible (same result on every run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e61b04e1",
      "metadata": {
        "id": "e61b04e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "9cb162c0-fe97-41b8-da3e-0d361529a94b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1845594194.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;31m# Note: If your model is poorly designed or performs poorly, points may be deducted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;31m# Check if the model can predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1845594194.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDT_ensemble\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# list of tuple where i is the classifier and j is the parameters (as a dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratified_kf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m       \u001b[0mgrid_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m       \u001b[0;31m# Update best_score_ and best_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# TASK 3\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=41)\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"\n",
        "    TODO: Train and return a DT classifier.\n",
        "\n",
        "    Args:\n",
        "        X_train: Training feature vectors\n",
        "        y_train: Training labels\n",
        "\n",
        "    Returns:\n",
        "        A trained sklearn model, your model will be used to predict the labels of test data\n",
        "    \"\"\"\n",
        "\n",
        "    model = None\n",
        "\n",
        "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
        "    # Import packages\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.ensemble import AdaBoostClassifier\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    from sklearn.ensemble import BaggingClassifier\n",
        "    from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "\n",
        "    # Not much data preprocessing is needed for decision trees (standardising or outlier removal), so it is likely that most of the improvement will come from hyperparameter tuning\n",
        "    # Just do the basic check for missing values\n",
        "\n",
        "    # print(type(X_train[0][1])) # X_train is a nested array of float type inside\n",
        "    # print(type(y_train[0])) # y_train is an array of int type\n",
        "    # When i say array it refers to the numpy array, which I CANNOT use :(\n",
        "\n",
        "    # print(len(X_train)) Only 105\n",
        "    # print(len(y_train)) As expected also same at 105\n",
        "    # So small sample size\n",
        "\n",
        "    # Same code from assignment 1\n",
        "    ## Detect and drop missing values if there are any using None, NA, Na, null not 0\n",
        "    count_X = 0\n",
        "    count_y = 0\n",
        "\n",
        "    for sub_list in X_train:\n",
        "      for i in sub_list:\n",
        "        if i is None or i == 'NA' or i == 'Na' or i == 'null':\n",
        "          count_X += 1\n",
        "        else:\n",
        "          pass\n",
        "\n",
        "    for j in y_train:\n",
        "      if j is None or j == 'NA' or j == 'Na' or j == 'null':\n",
        "        count_y += 1\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "    # print(count_X) # 0\n",
        "    # print(count_y) # 0\n",
        "    # As expected the iris data set has no missing data\n",
        "    # End of checking for missing values\n",
        "\n",
        "    # Idea use different ensembles for DT such as: 1. Random Forest, 2. Adaboost, 3. Bagging, 4. GradientBoosting (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
        "    ### Checked with prof, we can use ensembles\n",
        "    # We use GridSearchCV and loop for all of them, keeping the best model\n",
        "    # IMPORTANT: Based on task description, it is a DT classifier NOT regressor\n",
        "    # DecisionTree, decided to include it just for completeness\n",
        "\n",
        "    best_score = 0\n",
        "    best_model = None\n",
        "    best_params = None\n",
        "\n",
        "    DT_ensemble = [\n",
        "        # RandomForest (RF) from https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "        # Decided to try random_state = 3244 instead of 42 for once\n",
        "        (RandomForestClassifier(random_state = 3244), {\n",
        "            'n_estimators': [50, 100, 200, 300],\n",
        "            'max_depth': [None, 2, 3, 4, 5, 6],\n",
        "            'min_samples_split': [2, 3, 4, 5], # default is 2\n",
        "            'min_samples_leaf': [1, 2, 3], # default is 1\n",
        "            'max_features': ['sqrt', 'log2', None],\n",
        "            'bootstrap': [True, False],\n",
        "            'class_weight': [None, 'balanced']\n",
        "        }),\n",
        "        # AdaBoost from https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html, honestly not much to tune\n",
        "        (AdaBoostClassifier(random_state = 3244), {\n",
        "            'n_estimators': [50, 100, 200, 300],\n",
        "            'learning_rate': [0.01, 0.1, 0.5, 1.0]\n",
        "        }),\n",
        "        # GradientBoosting from https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#gradientboostingclassifier\n",
        "        (GradientBoostingClassifier(random_state = 3244), {\n",
        "            'n_estimators': [50, 100, 200, 300],\n",
        "            'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
        "            'max_depth': [None, 2, 3, 4, 5, 6],\n",
        "            'min_samples_split': [2, 3, 4, 5],\n",
        "            'min_samples_leaf': [1, 2, 3],\n",
        "            'max_features': ['sqrt', 'log2', None],\n",
        "            'subsample': [0.8, 1.0] # subsample < 1.0 is stochastic gradientboosting\n",
        "        }),\n",
        "        # Bagging from https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
        "        (BaggingClassifier(random_state = 3244), {\n",
        "            'n_estimators': [50, 100, 200, 300],\n",
        "            'max_samples': [0.5, 1], # If int, then draw max_samples samples. If float, then draw max_samples * X.shape[0] samples.\n",
        "            'max_features': [0.5, 1], # similar deal to max_samples\n",
        "            'bootstrap': [True, False],\n",
        "            'bootstrap_features': [True, False] # Whether features are drawn with replacement.\n",
        "        }),\n",
        "        # DecisionTree, decided to include it just for completeness from https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "        (DecisionTreeClassifier(random_state = 3244), {\n",
        "            'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "            'max_depth': [None, 2, 3, 4, 5, 6],\n",
        "            'min_samples_split': [2, 3, 4, 5], # default is 2\n",
        "            'min_samples_leaf': [1, 2, 3], # default is 1\n",
        "            'max_features': ['sqrt', 'log2', None],\n",
        "            'class_weight': [None, 'balanced']\n",
        "        })\n",
        "    ]\n",
        "\n",
        "    # Initialize StratifiedKFold\n",
        "    stratified_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=3244)\n",
        "\n",
        "    for i, j in DT_ensemble: # list of tuple where i is the classifier and j is the parameters (as a dict)\n",
        "      grid_search = GridSearchCV(estimator=i, param_grid = j, cv=stratified_kf, n_jobs = -1, scoring = \"accuracy\")\n",
        "      grid_fit = grid_search.fit(X_train, y_train)\n",
        "\n",
        "      # Update best_score_ and best_model\n",
        "      if grid_search.best_score_ > best_score:\n",
        "        best_score = grid_search.best_score_\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "        # Find best hyperparameters so that we don't need to run the code for another 2h+\n",
        "        best_params = grid_search.best_params_ # Hyperparameters that gave the best model performance\n",
        "\n",
        "        model = best_model\n",
        "\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "    print(\"Best Hyperparameters:\", best_params)\n",
        "    print(\"Best model:\", best_model)\n",
        "    print(\"Best Score:\", best_score)\n",
        "\n",
        "    ## Manual process to run model using best hyperparameter without gridsearch (Uncomment this and comment out gridsearch if you don't want to wait 1 hour+ when testing)\n",
        "\n",
        "    # model = BaggingClassifier(bootstrap=False, bootstrap_features=True, max_features=0.5, max_samples=0.5, n_estimators=50, random_state=3244)\n",
        "    # model.fit(X_train, y_train)\n",
        "\n",
        "    ## End of manual process to run model using best hyperparameter\n",
        "\n",
        "    # First run attempt, had to stop\n",
        "\n",
        "    # Second run attempt, ran out of free computing units so I had to switch to running on my local computer\n",
        "\n",
        "    # Test 3\n",
        "    # Best Hyperparameters: {'bootstrap': False, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 50}\n",
        "    # Best model: BaggingClassifier(bootstrap=False, bootstrap_features=True, max_features=0.5,\n",
        "    #               max_samples=0.5, n_estimators=50, random_state=3244)\n",
        "    # Best Score: 0.980952380952381\n",
        "    # Model accuracy: 0.89\n",
        "\n",
        "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# TESTCASES 3\n",
        "# Our hidden test cases will use your code to train a model to predict the labels of the test data, not necessarily on the same train-test split.\n",
        "# Note: If your model is poorly designed or performs poorly, points may be deducted.\n",
        "\n",
        "model = train_model(X_train, y_train)\n",
        "# Check if the model can predict\n",
        "predictions = model.predict(X_test)\n",
        "assert len(predictions) == len(X_test)\n",
        "accuracy_score = model.score(X_test, y_test)\n",
        "print(f\"Model accuracy: {accuracy_score:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40cea014",
      "metadata": {
        "id": "40cea014"
      },
      "source": [
        "## END OF ASSIGNMENT"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}