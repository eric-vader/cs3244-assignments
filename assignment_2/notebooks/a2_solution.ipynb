{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38d7d28",
   "metadata": {},
   "source": [
    "# Assignment 2: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19dc9a3",
   "metadata": {},
   "source": [
    "Fill in your name and student ID here.\n",
    "- Name: \n",
    "- Student ID: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e05743",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804e854",
   "metadata": {},
   "source": [
    "In this assignment, we'll implement Decision Trees:\n",
    "\n",
    "1. **Decision Tree Regression**\n",
    "    - Compute Region-Residual Sum of Squares (Region-RSS)\n",
    "    - Decision Tree Regressor\n",
    "2. **Decision Tree Classification**\n",
    "    - Calculate Entropy\n",
    "    - Compute Information Gain\n",
    "    - Implement the Majority Class function\n",
    "    - Implement the Best Split function\n",
    "    - Implement the Recursive Tree Builder Function\n",
    "    - Implement Prediction Logic for a single data point\n",
    "    - Decision Tree Classifier\n",
    "3. **Practical**: Train a DT classifier on the training dataset using scikit-learn\n",
    "\n",
    "By the end, you'll understand how DT works and how to tackle a problem using DT. Letâ€™s dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b23d61",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36bcc6",
   "metadata": {},
   "source": [
    "1. Fill in your name and student ID at the top of the ipynb file.\n",
    "2. The parts you need to implement are clearly marked with the following:\n",
    "\n",
    "    ```\n",
    "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "\n",
    "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "    ```\n",
    "\n",
    "    , and you must **ONLY** write your code in between the above two lines. \n",
    "3. **IMPORTANT**: Make sure that all of the cells are runnable and can compile without exception, even if the answer is incorrect. This will significantly help us in grading your solutions.\n",
    "3. For task 1 and 2, you are only allowed to use basic Python functions in your code (no `NumPy` or its equivalents), unless otherwise stated. You may reuse any functions you have defined earlier. If you are unsure whether a particular function is allowed, feel free to ask any of the TAs.\n",
    "4. For task 3, you may use the `scikit-learn` library.\n",
    "5. Your solutions will be evaluated against a set of hidden test cases to prevent hardcoding of the answer. You may assume that the test cases are always valid and does not require exception or input mismatch handling. Partial marks may be given for partially correct solutions\n",
    "\n",
    "### Submission Instructions\n",
    "Items to be submitted:\n",
    "* **This notebook, NAME-STUID-assignment2.ipynb**: This is where you fill in all your code. Replace \"NAME\" with your full name and \"STUID\" with your student ID, which starts with \"A\", e.g. `\"John Doe-A0123456X-assignment2.ipynb\"`\n",
    "\n",
    "Submit your assignment by **Sunday, 14 September 23:59** to Canvas. Points will be deducted late submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fadaa-a686-498a-b9c0-d56a6c8b591f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d142c",
   "metadata": {},
   "source": [
    "## Task 1 - Decision Tree Regression [4 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef572c3-37d7-4cb1-a9b3-e3c5c35f2586",
   "metadata": {},
   "source": [
    "### Task 1.1 - Compute Region-Residual Sum of Squares (Region-RSS) [1 Point]\n",
    "\n",
    "Minimizing $\\operatorname{Region-RSS}(l, c)$ helps us find the best split for the decision tree at each step.\n",
    "\n",
    "$$\n",
    "\\underbrace{\\operatorname{Region-RSS}(l,c)}_\\text{Assume feature $l$, cutoff $c$}= \\underbrace{\\operatorname{RSS}(\\{X|X_l<c\\})}_\\text{Left subregion} + \\underbrace{\\operatorname{RSS}(\\{X|X_l\\geq c\\})}_\\text{Right subregion}\n",
    "$$\n",
    "\n",
    "In order to do so, we need to calculate the residuals for each sub-region:\n",
    "\n",
    "$$\n",
    "\\operatorname{RSS}(X) = \\sum_{i=1}^{n} \\left(y_i - \\hat{f}(x_i)\\right)^2 = \\sum_{i=1}^{n} \\left(e_i\\right)^2\n",
    "$$\n",
    "\n",
    "Implement a function that computes the Region-RSS for a given cutoff, using the target values in the left and right sub-regions (`y_left` and `y_right`), without the use of `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea197b46-1d86-4dbd-8474-36d62b8291d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "# TASK 1.1\n",
    "def calculate_regionrss(y_left, y_right):\n",
    "    \"\"\"\n",
    "    TODO: Compute the Region-RSS for the split.\n",
    "    Avoid using NumPy and use only basic Python functions.\n",
    "\n",
    "    Args:\n",
    "        y_left: A list of target values in the left split\n",
    "        y_right: A list of target values in the right split\n",
    "\n",
    "    Returns:\n",
    "        Region-RSS error for a given cutoff\n",
    "    \"\"\"\n",
    "\n",
    "    total_rss = 0\n",
    "\n",
    "    \"\"\" YOUR CODE STARTS HERE \"\"\" \n",
    "    def rss(y):\n",
    "        if len(y) == 0:\n",
    "            return 0.0\n",
    "        mean_y = sum(y) / len(y)\n",
    "        squared_differences = [(value - mean_y) ** 2 for value in y]\n",
    "        return sum(squared_differences)\n",
    "\n",
    "    total_rss = rss(y_left) + rss(y_right)\n",
    "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "    \n",
    "    return total_rss\n",
    "\n",
    "# TESTCASES 1.1\n",
    "import math\n",
    "\n",
    "assert math.isclose(calculate_regionrss([3, 4, 5], [8, 9]), 2.5, rel_tol=1e-5)\n",
    "assert math.isclose(calculate_regionrss([1, 1, 1], [1, 1]), 0.0, rel_tol=1e-5)\n",
    "assert math.isclose(calculate_regionrss([], [2, 4, 6]), 8.0, rel_tol=1e-5)    \n",
    "print('All test cases passed!') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f06f5",
   "metadata": {},
   "source": [
    "### Task 1.2 - Decision Tree Regressor [3 Points]\n",
    "\n",
    "Our Decision Tree Regressor recursively splits the data into two regions at each node, using binary splits that minimize the Region-RSS. Splitting stops when the maximum depth is reached or further splitting is not possible. Each leaf predicts the mean target value of its region.\n",
    "\n",
    "You can call the provided `calculate_regionrss` function directly (on Coursemology), without copy-pasting, to evaluate cutoff quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101dbdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "# TASK 1.2\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None  \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: A list of feature values (one feature per sample).\n",
    "            y: A list of target values corresponding to each sample.\n",
    "        \"\"\"\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        TODO: Implement the recursive tree building logic using RSS.\n",
    "        Hint: Add to right node when it is exactly the same as the split value.\n",
    "\n",
    "        Args:\n",
    "            X: A list of feature values (one feature per sample)\n",
    "            y: A list of target values corresponding to each sample\n",
    "            depth: The current depth of the tree\n",
    "\n",
    "        Returns:\n",
    "            dict or float:\n",
    "                If it's an internal node, returns a dictionary representing the node:\n",
    "                - 'split_value': The feature value at which the split occurs.\n",
    "                - 'left': The left child node (recursively built tree structure or leaf value).\n",
    "                - 'right': The right child node (recursively built tree structure or leaf value).\n",
    "                If it's a leaf node (base case) or max_depth reached, returns the mean of the target values\n",
    "                in that region, which will be the prediction for that leaf.\n",
    "        \"\"\"\n",
    "        \n",
    "        res = None\n",
    "\n",
    "        \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "        unique_X = sorted(set(X))\n",
    "        # Base case: stop if max depth reached or nothing to split\n",
    "        if depth == self.max_depth or len(unique_X) <= 1:\n",
    "            return sum(y) / len(y)  # Predict the mean of the targets\n",
    "        \n",
    "        best_rss = float('inf')\n",
    "        best_split = None\n",
    "        \n",
    "        # Try all possible split points\n",
    "        for i in range(1, len(unique_X)):\n",
    "            split_value = (unique_X[i - 1] + unique_X[i]) / 2\n",
    "            left_y = [y[j] for j in range(len(X)) if X[j] < split_value]\n",
    "            right_y = [y[j] for j in range(len(X)) if X[j] >= split_value]\n",
    "            \n",
    "            current_rss = calculate_regionrss(left_y, right_y)\n",
    "            \n",
    "            if current_rss < best_rss:\n",
    "                best_rss = current_rss\n",
    "                best_split = split_value\n",
    "        \n",
    "        if best_split is None:\n",
    "            return sum(y) / len(y)  # Fallback to mean\n",
    "        \n",
    "        # Partition the data using the best split\n",
    "        left_X = [X[i] for i in range(len(X)) if X[i] < best_split]\n",
    "        left_y = [y[i] for i in range(len(X)) if X[i] < best_split]\n",
    "        right_X = [X[i] for i in range(len(X)) if X[i] >= best_split]\n",
    "        right_y = [y[i] for i in range(len(X)) if X[i] >= best_split]\n",
    "        \n",
    "        res = {\n",
    "            'split_value': best_split,\n",
    "            'left': self._build_tree(left_X, left_y, depth + 1),\n",
    "            'right': self._build_tree(right_X, right_y, depth + 1)\n",
    "        }\n",
    "        \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "\n",
    "        return res\n",
    "\n",
    "    def predict_one(self, x, node=None):\n",
    "        \"\"\"\n",
    "        TODO: Traverse the tree to make a prediction for a single input.\n",
    "        Hint: Prediction should go to the right child node when it is exactly the same as the split value.\n",
    "\n",
    "        Args:\n",
    "            x: A single input feature value.\n",
    "            node: The current node in the tree (used for recursion)\n",
    "\n",
    "        Returns:\n",
    "            A float representing the predicted target value for the input\n",
    "        \"\"\"\n",
    "\n",
    "        prediction = 0\n",
    "\n",
    "        \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "\n",
    "        if not isinstance(node, dict):\n",
    "            return node  # Leaf node\n",
    "        \n",
    "        if x < node['split_value']:\n",
    "            prediction = self.predict_one(x, node['left'])\n",
    "        else:\n",
    "            prediction = self.predict_one(x, node['right'])\n",
    "        \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        TODO: Call predict_one for each input in X and return the predictions.\n",
    "\n",
    "        Args:\n",
    "            X: A list of input feature values (one feature per sample)\n",
    "\n",
    "        Returns:\n",
    "            A list of predicted target values\n",
    "        \"\"\"\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "        predictions = [self.predict_one(x) for x in X]\n",
    "        \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# TESTCASES 1.2\n",
    "import math\n",
    "\n",
    "X = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=2)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict([1.5, 3.5, 5])\n",
    "assert all(isinstance(p, float) for p in predictions)\n",
    "assert len(predictions) == 3\n",
    "assert math.isclose(predictions[0], 4.0) \n",
    "assert math.isclose(predictions[1], 9.0) \n",
    "assert math.isclose(predictions[2], 9.0) \n",
    "\n",
    "\n",
    "X_offset = [0, 1, 2, 3]\n",
    "y_offset = [5, 7, 9, 11]\n",
    "model_offset = DecisionTreeRegressor(max_depth=2)\n",
    "model_offset.fit(X_offset, y_offset)\n",
    "predictions_offset = model_offset.predict([0.5, 2.5])\n",
    "assert all(isinstance(p, float) for p in predictions_offset)\n",
    "assert len(predictions_offset) == 2\n",
    "assert math.isclose(predictions_offset[0], 7.0) \n",
    "assert math.isclose(predictions_offset[1], 11.0) \n",
    "\n",
    "print('All test cases passed!') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ae4dc",
   "metadata": {},
   "source": [
    "## Task 2 - Decision Tree Classification [11 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f898a9d",
   "metadata": {},
   "source": [
    "### Task 2.1 - Calculate Entropy [1 Point]\n",
    "\n",
    "The entropy of a label set quantifies the amount of uncertainty or impurity in the distribution of class labels. It is defined as:\n",
    "\n",
    "$$\n",
    "H(E) = -\\sum_{e\\in E}{P\\left(e\\right)\\log_{|E|}\\left(P\\left(e\\right)\\right)}\n",
    "$$\n",
    "\n",
    "where $P(e)$ is the proportion of samples belonging to class $e$, and $|E|$ is the number of unique classes in the label set. Implement `compute_entropy` using the `math` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd37bb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.1\n",
    "import math\n",
    "\n",
    "def compute_entropy(y, n_unique_classes):\n",
    "    \"\"\"\n",
    "    TODO: Compute class proportions and entropy using the formula.\n",
    "\n",
    "    Args:\n",
    "        y: A list of class labels (e.g., ['yes', 'no', 'yes', ...])\n",
    "        n_unique_classes: The number of unique classes in the dataset\n",
    "        \n",
    "    Returns:\n",
    "        A float representing the entropy of the label distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    entropy = 0\n",
    "\n",
    "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "    label_counts = {}\n",
    "    for label in y:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    \n",
    "    total = len(y)\n",
    "    entropy = 0.0\n",
    "    if n_unique_classes <= 1: \n",
    "        return 0.0\n",
    "\n",
    "    for count in label_counts.values():\n",
    "        p = count / total\n",
    "        entropy -= p * math.log(p, n_unique_classes)    # p is guaranteed to be positive here\n",
    "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "\n",
    "    return entropy\n",
    "\n",
    "# TESTCASES 2.1\n",
    "assert math.isclose(compute_entropy(['yes', 'no', 'yes', 'yes', 'no'], n_unique_classes=2), 0.970950, rel_tol=1e-5)\n",
    "assert math.isclose(compute_entropy(['yes', 'yes', 'yes','no','maybe','maybe','no','maybe'], n_unique_classes= 3), 0.985056, rel_tol=1e-5)\n",
    "assert math.isclose(compute_entropy(['cat', 'dog', 'cat', 'fish', 'dog', 'cat'], n_unique_classes= 3), 0.920619, rel_tol=1e-5)\n",
    "assert math.isclose(compute_entropy([], n_unique_classes=2), 0, rel_tol=1e-5)\n",
    "assert math.isclose(compute_entropy(['yes', 'yes', 'yes'], n_unique_classes=1), 0, rel_tol=1e-5)\n",
    "\n",
    "print('All test cases passed!')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06da2b9",
   "metadata": {},
   "source": [
    "### Task 2.2 - Compute Information Gain [1 Point]\n",
    "\n",
    "The information gain of a split of the attribute $A$ is:\n",
    "\n",
    "$$\n",
    "\\operatorname{IG}(D,A)=H(D) - \\sum_{v\\in A} {\\frac{|D_v|}{|D|}}H(D_v)=H(D) - \\sum_{v\\in A} P(D_v)H(D_v)\n",
    "$$\n",
    "\n",
    "Implement `information_gain` using the function `compute_entropy` defined before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ad3dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.2\n",
    "def information_gain(parent_y, list_of_child_ys, n_unique_classes=2):\n",
    "    \"\"\"\n",
    "    TODO: Compute the information gain from the parent to the children\n",
    "\n",
    "    Args:\n",
    "        parent_y: Target values of the parent node.\n",
    "        list_of_child_ys: A list where each element is a list of target values for a child node resulting from a split.\n",
    "        n_unique_classes: The number of unique classes in the dataset\n",
    "    \n",
    "    Returns:\n",
    "        A float representing the information gain from the split\n",
    "    \"\"\"\n",
    "    \n",
    "    gain = 0\n",
    "    \n",
    "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "    n = len(parent_y)\n",
    "    if n == 0:\n",
    "        return 0.0 # No gain if parent node is empty\n",
    "\n",
    "    entropy_parent = compute_entropy(parent_y, n_unique_classes)\n",
    "    weighted_entropy = 0.0\n",
    "\n",
    "    for child_y in list_of_child_ys:\n",
    "        weight = len(child_y) / n\n",
    "        weighted_entropy += weight * compute_entropy(child_y, n_unique_classes)\n",
    "    \n",
    "    gain = entropy_parent - weighted_entropy\n",
    "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "\n",
    "    return gain\n",
    "\n",
    "# TESTCASES 2.2\n",
    "import math\n",
    "\n",
    "parent = ['yes', 'no', 'yes', 'no']\n",
    "left = ['yes', 'yes']\n",
    "right = ['no', 'no']\n",
    "assert math.isclose(information_gain(parent, [left, right], n_unique_classes = 2), 1.0, rel_tol=1e-5)\n",
    "\n",
    "parent = ['yes', 'no', 'yes', 'no']\n",
    "left = ['yes', 'no']\n",
    "right = ['yes', 'no']\n",
    "assert math.isclose(information_gain(parent, [left, right], n_unique_classes = 2), 0.0, rel_tol=1e-5)\n",
    "\n",
    "\n",
    "parent = ['yes', 'no', 'yes', 'no', 'yes', 'no']\n",
    "child_1 = ['yes', 'yes']\n",
    "child_2 = ['no', 'no']\n",
    "child_3 = ['yes', 'no']\n",
    "assert math.isclose(information_gain(parent, [child_1, child_2, child_3], n_unique_classes=2), 2/3, rel_tol=1e-5)\n",
    "\n",
    "parent = []\n",
    "child_1 = []\n",
    "child_2 = []\n",
    "assert math.isclose(information_gain(parent, [child_1, child_2], n_unique_classes=2), 0, rel_tol=1e-5)\n",
    "\n",
    "print('All test cases passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9968096",
   "metadata": {},
   "source": [
    "### Task 2.3 Implement the Majority Class function [1 Point]\n",
    "\n",
    "This function finds the most frequent class label within a list of `y_labels`. When a new data point reaches a leaf node in a classification tree, it's assigned the class that's most common among the training examples in that leaf. This function determines that \"majority class.\" Count the occurrences of each label. If there's a tie for the highest count, return the smallest label (alphabetically first) to ensure consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2cb1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.3\n",
    "def majority_class(y_labels):\n",
    "        \"\"\" \n",
    "        TODO: Implement majority class calculation.\n",
    "\n",
    "        Args:\n",
    "            y: A list of class labels\n",
    "        \n",
    "        Returns:\n",
    "            The majority class label (the one with the highest count), or None if y_labels is empty\n",
    "            If there is a tie, return the smallest label\n",
    "        \"\"\"\n",
    "\n",
    "        majority_class = None\n",
    "        \n",
    "        \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "        counts = {}\n",
    "        for label in y_labels:\n",
    "            counts[label] = counts.get(label, 0) + 1\n",
    "        \n",
    "        if not counts:\n",
    "            return None \n",
    "\n",
    "        max_count = -1 \n",
    "        \n",
    "        # Sort labels to ensure deterministic tie-breaking (smallest label alphabetically)\n",
    "        sorted_labels = sorted(counts.keys())\n",
    "\n",
    "        for label in sorted_labels:\n",
    "            count = counts[label]\n",
    "            if count > max_count:\n",
    "                max_count = count\n",
    "                majority_class = label\n",
    "        \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "\n",
    "        return majority_class\n",
    "\n",
    "# TESTCASES 2.3\n",
    "y = ['yes', 'no', 'yes', 'no', 'yes']\n",
    "assert majority_class(y) == 'yes'\n",
    "\n",
    "y = ['yes', 'no', 'yes', 'no']\n",
    "assert majority_class(y) == 'no'\n",
    "\n",
    "y = [1, 2, 2, 3, 2]\n",
    "assert majority_class(y) == 2\n",
    "\n",
    "print('All test cases passed!') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d590c17b",
   "metadata": {},
   "source": [
    "### Task 2.4 - Implement the Best Split function [3 Points]\n",
    "\n",
    "Complete the `find_best_split` function below using the `information_gain` function defined before.\n",
    "\n",
    "For a given dataset `X` (features) and `y` (labels), it searches through all possible ways to split the data based on each feature and selects the best one A decision tree grows by finding splits that best separate the classes. This function chooses the split with the highest Information Gain (IG), leading to purer child nodes and better classification..\n",
    "\n",
    "This function must handle both categorical and continuous features:\n",
    "* Categorical Features\n",
    "    * String Values like `'red'`, `'blue'`, `'apple'`, etc.\n",
    "    * Perform a **multi-way split**, creating one child node per unique category.\n",
    "\n",
    "* Continuous Features\n",
    "    * Numeric values like `2.0`, `4.5`, etc.\n",
    "    * Perform **binary splits** at midpoints between consecutive sorted unique values.\n",
    "\n",
    "For each potential split:\n",
    "1. Split the data accordingly.\n",
    "2. Calculate the information gain from the split.\n",
    "3. Track the split details (feature index, split type, and child group values).\n",
    "\n",
    "Return the split with the highest information gain. If there is a tie in information gain, prefer the feature with the lowest index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f31b30d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.4\n",
    "def find_best_split(X, y, n_unique_classes=2):\n",
    "    \"\"\"\n",
    "    TODO: Implement the best split finding logic for both categorical and continuous features\n",
    "\n",
    "    Args:\n",
    "        X: List of feature vectors \n",
    "        y: List of target labels.\n",
    "        n_unique_classes: The total number of unique classes in the dataset, used as log base.\n",
    "\n",
    "    Returns:\n",
    "        A tuple:\n",
    "        (\n",
    "            best_gain,             # float: Highest information gain achieved by any split\n",
    "            best_feature_idx,      # int: Index of the feature that gives the best split\n",
    "            best_split_type,       # str: Either 'categorical' or 'continuous'\n",
    "            best_split_details     # dict: Structure varies based on split type (see below)\n",
    "        )\n",
    "\n",
    "        - For a categorical feature, best_split_details should be:\n",
    "            {\n",
    "                category_val_1: {'X': [...], 'y': [...]},\n",
    "                category_val_2: {'X': [...], 'y': [...]},\n",
    "                ...\n",
    "            }\n",
    "\n",
    "        - For a continuous feature, best_split_details should be:\n",
    "            {\n",
    "                'split_value': val,        # float: midpoint value used for binary split\n",
    "                'left_X': [...], 'left_y': [...],\n",
    "                'right_X': [...], 'right_y': [...]\n",
    "            }\n",
    "\n",
    "        If no valid split is found, return:\n",
    "            (-1.0, None, None, None)\n",
    "    \"\"\"\n",
    "\n",
    "    best_overall_gain = -1.0 \n",
    "    best_overall_feature_index = None\n",
    "    best_overall_split_type = None \n",
    "    best_overall_split_details = None\n",
    "\n",
    "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "    if len(X) == 0:\n",
    "        return (best_overall_gain, best_overall_feature_index, best_overall_split_type, best_overall_split_details)\n",
    "\n",
    "    n_features = len(X[0]) \n",
    "\n",
    "    for feature_idx in range(n_features):\n",
    "        feature_values_at_idx = [x[feature_idx] for x in X]\n",
    "        \n",
    "        is_categorical = isinstance(feature_values_at_idx[0], str)\n",
    "\n",
    "        current_feature_gain = -1.0\n",
    "        current_split_details = None\n",
    "\n",
    "        if is_categorical:\n",
    "            # --- Categorical feature: multi-way split ---\n",
    "            current_feature_categories_data = {} \n",
    "            unique_categories = set()\n",
    "\n",
    "            for i in range(len(X)):\n",
    "                category_val = X[i][feature_idx]\n",
    "                unique_categories.add(category_val)\n",
    "                if category_val not in current_feature_categories_data:\n",
    "                    current_feature_categories_data[category_val] = {'X': [], 'y': []}\n",
    "                current_feature_categories_data[category_val]['X'].append(X[i])\n",
    "                current_feature_categories_data[category_val]['y'].append(y[i])\n",
    "            \n",
    "            if len(unique_categories) <= 1:\n",
    "                continue\n",
    "\n",
    "            list_of_child_ys = [data['y'] for data in current_feature_categories_data.values()]\n",
    "            \n",
    "            # Pass n_unique_classes to information_gain\n",
    "            current_feature_gain = information_gain(y, list_of_child_ys, n_unique_classes)\n",
    "            current_split_details = current_feature_categories_data \n",
    "\n",
    "        else:\n",
    "            \n",
    "            if len(set(feature_values_at_idx)) == 1:\n",
    "                return (best_overall_gain, best_overall_feature_index, best_overall_split_type, best_overall_split_details)\n",
    "            \n",
    "            if len(set(y)) == 1:\n",
    "                continue\n",
    "\n",
    "            # --- Continuous feature: binary split ---\n",
    "            feature_data_pairs = sorted(zip(feature_values_at_idx, y))\n",
    "            \n",
    "            best_gain_for_this_continuous_feature = -1.0\n",
    "            current_continuous_split_details = None \n",
    "            for i in range(1, len(feature_data_pairs)):\n",
    "                if feature_data_pairs[i - 1][0] == feature_data_pairs[i][0]:\n",
    "                    continue\n",
    "                \n",
    "                split_value = (feature_data_pairs[i - 1][0] + feature_data_pairs[i][0]) / 2\n",
    "                \n",
    "                y_left_temp, y_right_temp = [], []\n",
    "                X_left_temp, X_right_temp = [], []\n",
    "\n",
    "                for sample_idx in range(len(X)):\n",
    "                    if X[sample_idx][feature_idx] < split_value:\n",
    "                        X_left_temp.append(X[sample_idx])\n",
    "                        y_left_temp.append(y[sample_idx])\n",
    "                    else:\n",
    "                        X_right_temp.append(X[sample_idx])\n",
    "                        y_right_temp.append(y[sample_idx])\n",
    "                \n",
    "                if not y_left_temp or not y_right_temp:\n",
    "                    continue\n",
    "                \n",
    "                # Pass n_unique_classes to information_gain\n",
    "                gain_at_cutoff = information_gain(y, [y_left_temp, y_right_temp], n_unique_classes)\n",
    "                \n",
    "                if gain_at_cutoff > best_gain_for_this_continuous_feature:\n",
    "                    best_gain_for_this_continuous_feature = gain_at_cutoff\n",
    "                    # The split_value is implicitly stored in current_continuous_split_details\n",
    "                    current_continuous_split_details = {\n",
    "                        'split_value': split_value,\n",
    "                        'left_X': X_left_temp, 'left_y': y_left_temp,\n",
    "                        'right_X': X_right_temp, 'right_y': y_right_temp\n",
    "                    }\n",
    "            \n",
    "            current_feature_gain = best_gain_for_this_continuous_feature\n",
    "            current_split_details = current_continuous_split_details \n",
    "\n",
    "        # --- Unified Tie-Breaking and Best Split Selection ---\n",
    "        if current_feature_gain > best_overall_gain:\n",
    "            best_overall_gain = current_feature_gain\n",
    "            best_overall_feature_index = feature_idx\n",
    "            best_overall_split_type = 'categorical' if is_categorical else 'continuous'\n",
    "            best_overall_split_details = current_split_details\n",
    "        elif current_feature_gain == best_overall_gain:\n",
    "            # Tie-breaking: prefer lower feature index\n",
    "            if best_overall_feature_index is None or feature_idx < best_overall_feature_index:\n",
    "                best_overall_gain = current_feature_gain\n",
    "                best_overall_feature_index = feature_idx\n",
    "                best_overall_split_type = 'categorical' if is_categorical else 'continuous'\n",
    "                best_overall_split_details = current_split_details\n",
    "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "\n",
    "    return best_overall_gain, best_overall_feature_index, best_overall_split_type, best_overall_split_details\n",
    "\n",
    "# TESTCASES 2.4\n",
    "import math\n",
    "\n",
    "# TEST 1: Categorical Feature Split\n",
    "X = [['red'], ['blue'], ['red'], ['green'], ['blue']]\n",
    "y = ['yes', 'no', 'yes', 'no', 'no']\n",
    "n_unique_classes = 2\n",
    "gain, feature_idx, split_type, split_details = find_best_split(X, y, n_unique_classes)\n",
    "\n",
    "assert split_type == 'categorical'\n",
    "assert feature_idx == 0\n",
    "assert math.isclose(gain, 0.97095, rel_tol=1e-4)\n",
    "assert isinstance(split_details, dict)\n",
    "assert set(split_details.keys()) == {'red', 'blue', 'green'}\n",
    "\n",
    "# TEST 2: Continuous Feature Split\n",
    "X = [[2.0], [4.0], [6.0], [8.0], [10.0]]\n",
    "y = ['yes', 'yes', 'no', 'no', 'no']\n",
    "n_unique_classes = 2\n",
    "gain, feature_idx, split_type, split_details = find_best_split(X, y, n_unique_classes)\n",
    "\n",
    "assert split_type == 'continuous'\n",
    "assert feature_idx == 0\n",
    "assert isinstance(split_details, dict)\n",
    "assert 'split_value' in split_details\n",
    "assert split_details['split_value'] == 5.0  # Midpoint between 4.0 and 6.0\n",
    "assert math.isclose(gain, 0.97095, rel_tol=1e-4)\n",
    "\n",
    "# TEST 3: No Valid Split\n",
    "X = [['same'], ['same'], ['same']]\n",
    "y = ['yes', 'yes', 'yes']\n",
    "n_unique_classes = 1\n",
    "gain, feature_idx, split_type, split_details = find_best_split(X, y, n_unique_classes)\n",
    "\n",
    "assert gain == -1.0\n",
    "assert feature_idx is None\n",
    "assert split_type is None\n",
    "assert split_details is None\n",
    "\n",
    "print('All test cases passed!') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54923c12",
   "metadata": {},
   "source": [
    "### Task 2.5 - Implement the Recursive Tree Builder Function [2 Points]\n",
    "\n",
    "Complete the function `build_tree_recursive` below using the `find_best_split` and `majority_class` functions defined before.\n",
    "\n",
    "It works recursively to grow the tree branch by branch. At each step, it decides whether to stop growing (i.e., return a leaf node) or to split the data and create internal nodes with children. Use `majority_class` to label leaf nodes and `find_best_split` to decide the best way to divide data at internal nodes.\n",
    "\n",
    "Base Cases: Stop Recursion When Any of These is True:\n",
    "\n",
    "- The maximum depth (`max_depth`) is reached.\n",
    "- All `y` labels in the current subset are the same class (pure node).\n",
    "\n",
    "In these cases, return the majority class of the current `y` subset as the leaf node's prediction.\n",
    "\n",
    "Recursive Step: If No Base Case is Triggered\n",
    "\n",
    "1. Call `find_best_split(X, y, n_unique_classes)` to determine:\n",
    "   - Best feature index\n",
    "   - Type of feature (`'continuous'` or `'categorical'`)\n",
    "   - Split details (values and data partitions)\n",
    "\n",
    "2. Construct a dictionary representing an internal node, which includes:\n",
    "   - `'feature'`: index of the best splitting feature.\n",
    "   - `'split_type'`: `'continuous'` or `'categorical'`.\n",
    "   - For continuous features:\n",
    "     - `'split_value'`: float value for binary split.\n",
    "     - `'left'` and `'right'`: recursive child nodes.\n",
    "   - For categorical features:\n",
    "     - `'children_map'`: a dictionary mapping each category to a recursive child node.\n",
    "     - `'default_prediction'`: the majority class at this node (used as fallback during inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ff5783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test Case 1: Max depth reached\n",
      "Test Case 1 Passed: Correctly returned leaf value 0.\n",
      "------------------------------\n",
      "Running Test Case 2: All labels same\n",
      "Test Case 2 Passed: Correctly returned leaf value 0.\n",
      "------------------------------\n",
      "Running Test Case 3: Empty X\n",
      "Test Case 3 Passed: Correctly returned leaf value 0.\n",
      "------------------------------\n",
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.5\n",
    "def build_tree_recursive(X, y, depth, max_depth, n_unique_classes):\n",
    "    \"\"\"\n",
    "    TODO: Implement the recursive tree building logic using the best split found.\n",
    "\n",
    "    Args:\n",
    "        X: Current subset of feature vectors.\n",
    "        y: Current subset of target labels.\n",
    "        depth: Current depth of the tree.\n",
    "        max_depth: Maximum allowed depth for the tree.\n",
    "        n_unique_classes: The total number of unique classes in the dataset, used as log base.\n",
    "        \n",
    "    Returns:\n",
    "        Either:\n",
    "            - A leaf node: the majority class label (int) if the tree should stop splitting.\n",
    "            - OR a decision node (dict) with the following structure:\n",
    "\n",
    "        For continuous features:\n",
    "            {\n",
    "                'feature': <feature_index>,                # int, index of feature used for split\n",
    "                'split_type': 'continuous',               # str\n",
    "                'split_value': <threshold>,               # float, the midpoint used for binary split\n",
    "                'left': <left_subtree>,                   # recursive subtree or leaf\n",
    "                'right': <right_subtree>,                 # recursive subtree or leaf\n",
    "                'default_prediction': <majority_class>    # int, used for fallback predictions\n",
    "            }\n",
    "\n",
    "        For categorical features:\n",
    "            {\n",
    "                'feature': <feature_index>,                # int\n",
    "                'split_type': 'categorical',              # str\n",
    "                'children_map': {\n",
    "                    <category_val>: <subtree_or_leaf>,    # one child per category\n",
    "                    ...\n",
    "                },\n",
    "                'default_prediction': <majority_class>    # int\n",
    "            }\n",
    "    \"\"\"\n",
    "\n",
    "    res = None\n",
    "\n",
    "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "    # Base Cases:\n",
    "    if depth == max_depth or len(set(y)) == 1:\n",
    "        return majority_class(y)\n",
    "    \n",
    "    if not X:\n",
    "        return majority_class(y) \n",
    "\n",
    "    # Pass n_unique_classes to find_best_split\n",
    "    best_overall_gain, best_overall_feature_index, best_overall_split_type, best_overall_split_details = \\\n",
    "        find_best_split(X, y, n_unique_classes)\n",
    "    \n",
    "    if best_overall_gain <= 0: \n",
    "        return majority_class(y) \n",
    "\n",
    "    current_nodemajority_class = majority_class(y)\n",
    "\n",
    "    # Build child nodes based on the best overall split found\n",
    "    if best_overall_split_type == 'continuous':\n",
    "        res =  {\n",
    "            'feature': best_overall_feature_index,\n",
    "            'split_type': 'continuous',\n",
    "            'split_value': best_overall_split_details['split_value'],\n",
    "            # Pass n_unique_classes recursively\n",
    "            'left': build_tree_recursive(best_overall_split_details['left_X'], best_overall_split_details['left_y'], depth + 1, max_depth, n_unique_classes),\n",
    "            'right': build_tree_recursive(best_overall_split_details['right_X'], best_overall_split_details['right_y'], depth + 1, max_depth, n_unique_classes),\n",
    "            'default_prediction': current_nodemajority_class \n",
    "        }\n",
    "    else: # best_overall_split_type == 'categorical'\n",
    "        children_nodes = {}\n",
    "        for category_val, data in best_overall_split_details.items():\n",
    "            # Pass n_unique_classes recursively\n",
    "            children_nodes[category_val] = build_tree_recursive(data['X'], data['y'], depth + 1, max_depth, n_unique_classes)\n",
    "        \n",
    "        res = {\n",
    "            'feature': best_overall_feature_index,\n",
    "            'split_type': 'categorical',\n",
    "            'children_map': children_nodes, \n",
    "            'default_prediction': current_nodemajority_class \n",
    "        }\n",
    "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "    \n",
    "    return res\n",
    "\n",
    "# TESTCASES 2.5\n",
    "\n",
    "test_cases = [\n",
    "    # Test Case 1: Base Case - Max depth reached\n",
    "    {\n",
    "        \"name\": \"Max depth reached\",\n",
    "        \"X\": [[1], [2], [3]],\n",
    "        \"y\": [0, 1, 0],\n",
    "        \"depth\": 2,\n",
    "        \"max_depth\": 2,\n",
    "        \"n_unique_classes\": 2,\n",
    "        \"expected\": 0 # Majority class of [0, 1, 0] is 0\n",
    "    },\n",
    "    # Test Case 2: Base Case - All labels are the same\n",
    "    {\n",
    "        \"name\": \"All labels same\",\n",
    "        \"X\": [[1], [2], [3]],\n",
    "        \"y\": [0, 0, 0],\n",
    "        \"depth\": 0,\n",
    "        \"max_depth\": 5,\n",
    "        \"n_unique_classes\": 2,\n",
    "        \"expected\": 0 # Majority class of [0, 0, 0] is 0\n",
    "    },\n",
    "    # Test Case 3: Base Case - Empty X (should return majority of y)\n",
    "    {\n",
    "        \"name\": \"Empty X\",\n",
    "        \"X\": [],\n",
    "        \"y\": [0, 1, 0],\n",
    "        \"depth\": 0,\n",
    "        \"max_depth\": 5,\n",
    "        \"n_unique_classes\": 2,\n",
    "        \"expected\": 0 # Majority class of [0, 1, 0] is 0\n",
    "    }\n",
    "]\n",
    "\n",
    "# Helper function for running tests\n",
    "for i, tc in enumerate(test_cases):\n",
    "    print(f\"Running Test Case {i+1}: {tc['name']}\")\n",
    "    result = build_tree_recursive(tc['X'], tc['y'], tc['depth'], tc['max_depth'], tc['n_unique_classes'])\n",
    "\n",
    "    if \"expected_type\" in tc:\n",
    "        if tc[\"expected_type\"] == \"continuous_node\":\n",
    "            assert isinstance(result, dict) and result.get('split_type') == 'continuous', \\\n",
    "                f\"Test Case {i+1} Failed: Expected continuous node, got {result}\"\n",
    "            print(f\"Test Case {i+1} Passed: Correctly identified continuous node.\")\n",
    "        elif tc[\"expected_type\"] == \"categorical_node\":\n",
    "            assert isinstance(result, dict) and result.get('split_type') == 'categorical', \\\n",
    "                f\"Test Case {i+1} Failed: Expected categorical node, got {result}\"\n",
    "            print(f\"Test Case {i+1} Passed: Correctly identified categorical node.\")\n",
    "        elif tc[\"expected_type\"] == \"continuous_node_with_leaf_children\":\n",
    "            assert isinstance(result, dict) and result.get('split_type') == 'continuous', \\\n",
    "                f\"Test Case {i+1} Failed: Expected continuous node at root, got {result}\"\n",
    "            assert not isinstance(result['left'], dict) and not isinstance(result['right'], dict), \\\n",
    "                f\"Test Case {i+1} Failed: Expected leaf children, but found nodes. Left: {result['left']}, Right: {result['right']}\"\n",
    "            print(f\"Test Case {i+1} Passed: Correctly built continuous node with leaf children.\")\n",
    "    else:\n",
    "        assert result == tc['expected'], \\\n",
    "            f\"Test Case {i+1} Failed: Expected {tc['expected']}, got {result}\"\n",
    "        print(f\"Test Case {i+1} Passed: Correctly returned leaf value {result}.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print('All test cases passed!') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9d93e",
   "metadata": {},
   "source": [
    "### Task 2.6  - Implement Prediction Logic for a single data point [2 Points]\n",
    "\n",
    "Complete the `predict_one_instance` function below. It takes a single data point and \"walks\" it down the decision tree to make a prediction.\n",
    "\n",
    "Starting from the root node:\n",
    "- Recursively check the `split_type` (`'continuous'` or `'categorical'`) and the feature used at the current `tree_node`.\n",
    "- Based on `x_instance`'s value for that feature, decide:\n",
    "  - whether to go `left` or `right` (for continuous),\n",
    "  - or which `category` branch to follow (for categorical).\n",
    "\n",
    "Continue this process until a leaf node, which contains the final predicted class.\n",
    "\n",
    "Base Case:\n",
    "\n",
    "- If `tree_node` is not a dictionary, it's a leaf node.\n",
    "- Return its value directly (the predicted class).\n",
    "\n",
    "Traversal Logic:\n",
    "\n",
    "*   Continuous Split:\n",
    "    - Compare `x_instance[feature_idx]` with `tree_node['split_value']`.\n",
    "    - If less, recurse into `tree_node['left']`; else recurse into `tree_node['right']`.\n",
    "\n",
    "*   Categorical Split:\n",
    "    - Look up `x_instance[feature_idx]` in `tree_node['children_map']`.\n",
    "    - If found, recurse into the matching child.\n",
    "    - If unseen category, return `tree_node['default_prediction']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ff14d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1 Passed (build & continuous prediction left)\n",
      "Test Case 1 Passed (build & continuous prediction right)\n",
      "Test Case 2 Passed (build & categorical prediction 'apple')\n",
      "Test Case 2 Passed (build & categorical prediction 'grape' - default)\n",
      "Test Case 3 Passed (mixed features - continuous path)\n",
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.6\n",
    "def predict_one_instance(x_instance, tree_node):\n",
    "    \"\"\"\n",
    "    TODO: Traverse the decision tree to make a prediction for a single input instance.\n",
    "\n",
    "    Args:\n",
    "        x_instance: A single feature vector (e.g., [val1, val2])\n",
    "        tree_node: The current node of the decision tree (starts with the root)\n",
    "    \n",
    "    Returns:\n",
    "        The predicted class label\n",
    "    \"\"\"\n",
    "\n",
    "    predicted_label = None\n",
    "\n",
    "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "    if not isinstance(tree_node, dict): # Leaf node (contains the predicted class directly)\n",
    "        return tree_node\n",
    "    \n",
    "    # Check if the feature index exists in the input 'x_instance'\n",
    "    feature_idx = tree_node['feature']\n",
    "    feature_val = x_instance[feature_idx]\n",
    "\n",
    "    if tree_node['split_type'] == 'continuous':\n",
    "        split_value = tree_node['split_value']\n",
    "        if feature_val < split_value:\n",
    "            predicted_label = predict_one_instance(x_instance, tree_node['left'])\n",
    "        else:\n",
    "            predicted_label = predict_one_instance(x_instance, tree_node['right'])\n",
    "    \n",
    "    else: # tree_node['split_type'] == 'categorical':\n",
    "        # Use .get() with the default_prediction as fallback for unseen categories\n",
    "        next_node = tree_node['children_map'].get(feature_val)\n",
    "        if next_node is not None:\n",
    "             predicted_label = predict_one_instance(x_instance, next_node)\n",
    "        else:\n",
    "             # Unseen category encountered, use the default prediction for this node\n",
    "             predicted_label = tree_node['default_prediction']\n",
    "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# TESTCASES 2.6\n",
    "\n",
    "# Test Case 1: Build a simple tree with a continuous split and predict.\n",
    "X_tc1 = [[3.0], [7.0], [2.0], [8.0]]\n",
    "y_tc1 = [0, 1, 0, 1]\n",
    "n_unique_classes_tc1 = 2\n",
    "max_depth_tc1 = 1 \n",
    "tree_tc1 = build_tree_recursive(X_tc1, y_tc1, 0, max_depth_tc1, n_unique_classes_tc1)\n",
    "# Predict an instance that goes left\n",
    "prediction_tc1_left = predict_one_instance([2.5], tree_tc1)\n",
    "assert prediction_tc1_left == 0, f\"Test Case 1 Failed (left prediction): Expected 0, got {prediction_tc1_left}\"\n",
    "print(\"Test Case 1 Passed (build & continuous prediction left)\")\n",
    "# Predict an instance that goes right\n",
    "prediction_tc1_right = predict_one_instance([6.0], tree_tc1)\n",
    "assert prediction_tc1_right == 1, f\"Test Case 1 Failed (right prediction): Expected 1, got {prediction_tc1_right}\"\n",
    "print(\"Test Case 1 Passed (build & continuous prediction right)\")\n",
    "\n",
    "\n",
    "# Test Case 2: Build a simple tree with a categorical split and predict.\n",
    "X_tc2 = [['apple'], ['banana'], ['apple'], ['orange']]\n",
    "y_tc2 = [0, 1, 0, 1]\n",
    "n_unique_classes_tc2 = 2\n",
    "max_depth_tc2 = 1\n",
    "tree_tc2 = build_tree_recursive(X_tc2, y_tc2, 0, max_depth_tc2, n_unique_classes_tc2)\n",
    "# Predict a known category ('apple')\n",
    "prediction_tc2_apple = predict_one_instance(['apple'], tree_tc2)\n",
    "assert prediction_tc2_apple == 0, f\"Test Case 2 Failed (apple prediction): Expected 0, got {prediction_tc2_apple}\"\n",
    "print(\"Test Case 2 Passed (build & categorical prediction 'apple')\")\n",
    "# Predict an unknown category ('grape') - should use default_prediction\n",
    "prediction_tc2_grape = predict_one_instance(['grape'], tree_tc2)\n",
    "assert prediction_tc2_grape == majority_class(y_tc2), f\"Test Case 2 Failed (grape prediction): Expected default {majority_class(y_tc2)}, got {prediction_tc2_grape}\"\n",
    "print(\"Test Case 2 Passed (build & categorical prediction 'grape' - default)\")\n",
    "\n",
    "# Test Case 3: Mixed features - build a tree, and predict a continuous feature value path.\n",
    "X_tc3 = [[10, 'A'], [2, 'B'], [12, 'A'], [3, 'B']]\n",
    "y_tc3 = [0, 1, 0, 1]\n",
    "n_unique_classes_tc3 = 2\n",
    "max_depth_tc3 = 1\n",
    "tree_tc3 = build_tree_recursive(X_tc3, y_tc3, 0, max_depth_tc3, n_unique_classes_tc3)\n",
    "# Predict an instance (continuous feature 0 value: 4, which is <= 5)\n",
    "prediction_tc3 = predict_one_instance([4, 'C'], tree_tc3) # The 'C' doesn't matter for this split\n",
    "assert prediction_tc3 == 1, f\"Test Case 3 Failed: Expected 1, got {prediction_tc3}\" # Majority of [1,1] from left branch if split at 5\n",
    "print(\"Test Case 3 Passed (mixed features - continuous path)\")\n",
    "\n",
    "print('All test cases passed!') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba8c6b",
   "metadata": {},
   "source": [
    "### Task 2.7 - Decision Tree Classifier [1 Point]\n",
    "\n",
    "Complete the class below using the `majority_class`, `build_tree_recursive` and `predict_one_instance` functions defined before. We have already imported them for you on Coursemology.\n",
    "\n",
    "This class brings together all the previous helper functions to create, train, and use a decision tree for classification tasks.\n",
    "\n",
    "* `fit(self, X, y)`:\n",
    "    *   Calculate and store the total number of unique classes from `y` in `self.n_unique_classes`.\n",
    "    *   Handle edge cases: If there's only one (or zero) unique class, the tree should simply be a leaf node representing that `majority_class`.\n",
    "    *   Otherwise, call `build_tree_recursive` to start building the tree, passing all necessary parameters including `self.max_depth` and `self.n_unique_classes`. Store the resulting tree structure in `self.tree`.\n",
    "\n",
    "*   `predict(self, X)`:\n",
    "    *   If `self.tree` is a leaf node (not a dictionary), simply return a list of that leaf value repeated for each instance in `X`.\n",
    "    *   Otherwise, iterate through each `x_instance` in the input `X` and call `predict_one_instance` to get a prediction for each. Collect these predictions into a list and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d24c495d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "# TASK 2.7\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_depth (int): The maximum depth of the tree.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None \n",
    "        self.n_unique_classes = None # To store the total number of unique classes\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO: Fit the decision tree to the training data. DO NOT return anything!\n",
    "\n",
    "        Args:\n",
    "            X: A list of feature vectors for training.\n",
    "            y: A list of corresponding target labels.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "        self.n_unique_classes = len(set(y))\n",
    "\n",
    "        # Build the tree using the recursive builder function, passing n_unique_classes\n",
    "        self.tree = build_tree_recursive(X, y, depth=0, max_depth=self.max_depth, n_unique_classes=self.n_unique_classes)\n",
    "        \n",
    "        \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        TODO: Predict class labels for each instance in X using the fitted tree\n",
    "\n",
    "        Args:\n",
    "            X: A list of feature vectors for prediction.\n",
    "\n",
    "        Returns:\n",
    "            A list of predicted class labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        predicted_labels = []\n",
    "\n",
    "        \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "        # If the tree is just a leaf node (e.g., all training data was same class)\n",
    "        if not isinstance(self.tree, dict):\n",
    "            return [self.tree for _ in X]\n",
    "\n",
    "        predicted_labels = [predict_one_instance(x_instance, self.tree) for x_instance in X]\n",
    "        \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "\n",
    "        return predicted_labels\n",
    "        \n",
    "# TESTCASES 2.7\n",
    "\n",
    "# Test Case 1\n",
    "X = [[1.0], [2.0], [3.0]]\n",
    "y = [0, 0, 0]\n",
    "clf = DecisionTreeClassifier(max_depth=5)\n",
    "clf.fit(X, y)\n",
    "X_predict = [[10.0], [20.0]]\n",
    "predictions = clf.predict(X_predict)\n",
    "assert predictions == [0,0] , f\"Prediction for a single-leaf tree should return that leaf's value for all instances.\"\n",
    "\n",
    "# Test Case 2\n",
    "X_train = [[3.0], [7.0], [2.0], [8.0]]\n",
    "y_train = [0, 1, 0, 1]\n",
    "max_depth = 1 \n",
    "clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "clf.fit(X_train, y_train)\n",
    "X_predict = [[2.5], [6.0], [4.9], [5.1]] \n",
    "predictions = clf.predict(X_predict)\n",
    "assert predictions == [0, 1, 0, 1] , f\"Prediction for continuous features should correctly traverse the tree.\"\n",
    "\n",
    "# Test Case 3\n",
    "X_train = [['red'], ['blue'], ['red'], ['green']]\n",
    "y_train = [0, 1, 0, 1]\n",
    "max_depth = 1 \n",
    "clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "clf.fit(X_train, y_train)\n",
    "X_predict = [['red'], ['blue'], ['yellow']] \n",
    "predictions = clf.predict(X_predict)\n",
    "assert predictions == [0, 1, majority_class(y_train)] , f\"Prediction for categorical features should correctly use children_map and handle unseen categories.\"\n",
    "\n",
    "print('All test cases passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd5f4b",
   "metadata": {},
   "source": [
    "## Task 3 - Practical [2 Points]\n",
    "\n",
    "Train a DT classifier on the training dataset using `scikit-learn` and tune its hyperparameters to optimize performance. \n",
    "\n",
    "You will get full marks if your modelling is appropriate and performs well. But remember, you **MUST NOT** use or access X_test and y_test in your code, as this defeats the purpose of a hidden test set. Any model that does so will be given 0 mark.\n",
    "\n",
    "Make sure that you have installed `scikit-learn` in your python environment. \n",
    "\n",
    "**HINT**: Set the `random_state` parameter (if exists) to a certain constant to make your model reproducible (same result on every run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e61b04e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "# TASK 3\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=41)\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    TODO: Train and return a DT classifier.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training feature vectors\n",
    "        y_train: Training labels\n",
    "\n",
    "    Returns:\n",
    "        A trained sklearn model, your model will be used to predict the labels of test data\n",
    "    \"\"\"\n",
    "    \n",
    "    model = None\n",
    "\n",
    "    \"\"\" YOUR CODE STARTS HERE \"\"\"\n",
    "    model = make_pipeline(\n",
    "        DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \"\"\" YOUR CODE ENDS HERE \"\"\"\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# TESTCASES 3\n",
    "# Our hidden test cases will use your code to train a model to predict the labels of the test data, not necessarily on the same train-test split.\n",
    "# Note: If your model is poorly designed or performs poorly, points may be deducted.\n",
    "\n",
    "model = train_model(X_train, y_train)\n",
    "# Check if the model can predict\n",
    "predictions = model.predict(X_test)\n",
    "assert len(predictions) == len(X_test)\n",
    "accuracy_score = model.score(X_test, y_test)\n",
    "print(f\"Model accuracy: {accuracy_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cea014",
   "metadata": {},
   "source": [
    "## END OF ASSIGNMENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
